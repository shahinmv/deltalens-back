{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, random\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinPredictor:\n",
    "    def __init__(self, sequence_length=60, prediction_horizon=30):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_groups = {\n",
    "            'price_volume': ['open', 'high', 'low', 'close', 'volume', 'high_close_ratio', \n",
    "                           'low_close_ratio', 'open_close_ratio', 'volume_avg_ratio', 'volume_change'],\n",
    "            'returns_momentum': ['returns_1d', 'returns_3d', 'returns_7d', 'log_returns', \n",
    "                               'momentum_5', 'momentum_10'],\n",
    "            'technical': ['ma_5', 'price_ma_5_ratio', 'ma_10', 'price_ma_10_ratio', 'ma_20', \n",
    "                         'price_ma_20_ratio', 'ema_12', 'ema_26', 'macd', 'macd_signal', \n",
    "                         'macd_normalized', 'macd_signal_normalized', 'rsi', 'rsi_normalized'],\n",
    "            'volatility': ['bb_middle', 'bb_upper', 'bb_lower', 'bb_position', 'bb_width', \n",
    "                          'volatility_10', 'volatility_20'],\n",
    "            'sentiment': ['avg_vader_compound', 'article_count', 'vader_ma_3', 'vader_ma_7', \n",
    "                         'article_count_norm'],\n",
    "            'funding': ['funding_rate', 'funding_rate_ma'],\n",
    "            'temporal': ['day_sin', 'day_cos']\n",
    "        }\n",
    "        \n",
    "    def engineer_30day_target(self, df):\n",
    "        \"\"\"Engineer 30-day forward returns target\"\"\"\n",
    "        df_target = df.copy()\n",
    "        # Calculate 30-day forward return\n",
    "        df_target['target_return_30d'] = (df_target['close'].shift(-self.prediction_horizon) - \n",
    "                                         df_target['close']) / df_target['close']\n",
    "        \n",
    "        # Optional: Add 30-day forward direction for classification\n",
    "        df_target['target_direction_30d'] = (df_target['target_return_30d'] > 0).astype(int)\n",
    "        \n",
    "        # Remove rows that don't have 30-day forward data\n",
    "        df_target = df_target.dropna()\n",
    "        return df_target\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Prepare and scale features\"\"\"\n",
    "        # Get all feature columns (exclude target and metadata)\n",
    "        feature_cols = []\n",
    "        for group_features in self.feature_groups.values():\n",
    "            feature_cols.extend(group_features)\n",
    "        \n",
    "        # Filter to only existing columns\n",
    "        available_features = [col for col in feature_cols if col in df.columns]\n",
    "        \n",
    "        # Add any additional features not in groups\n",
    "        additional_features = [col for col in df.columns if col not in available_features \n",
    "                             and col not in ['target_return_30d', 'target_direction_30d', \n",
    "                                           'next_close', 'target_return', 'target_direction']]\n",
    "        \n",
    "        all_features = available_features + additional_features\n",
    "        \n",
    "        print(f\"Using {len(all_features)} features: {all_features}\")\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
    "        scaled_features = self.scaler.fit_transform(df[all_features])\n",
    "        \n",
    "        return scaled_features, all_features\n",
    "    \n",
    "    def create_sequences(self, features, targets):\n",
    "        \"\"\"Create sequences for LSTM training\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(len(features) - self.sequence_length - self.prediction_horizon + 1):\n",
    "            X.append(features[i:(i + self.sequence_length)])\n",
    "            y.append(targets[i + self.sequence_length])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"Build hybrid CNN-LSTM model\"\"\"\n",
    "        # Input layer\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        \n",
    "        # CNN Branch - Extract local patterns\n",
    "        cnn_branch = layers.Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "                                  padding='same')(inputs)\n",
    "        cnn_branch = layers.Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "                                  padding='same')(cnn_branch)\n",
    "        cnn_branch = layers.MaxPooling1D(pool_size=2)(cnn_branch)\n",
    "        cnn_branch = layers.Dropout(0.2)(cnn_branch)\n",
    "        \n",
    "        cnn_branch = layers.Conv1D(filters=32, kernel_size=3, activation='relu', \n",
    "                                  padding='same')(cnn_branch)\n",
    "        cnn_branch = layers.Conv1D(filters=32, kernel_size=3, activation='relu', \n",
    "                                  padding='same')(cnn_branch)\n",
    "        cnn_branch = layers.GlobalMaxPooling1D()(cnn_branch)\n",
    "        \n",
    "        # LSTM Branch - Capture temporal dependencies\n",
    "        lstm_branch = layers.LSTM(100, return_sequences=True, dropout=0.2, \n",
    "                                 recurrent_dropout=0.2)(inputs)\n",
    "        lstm_branch = layers.LSTM(50, return_sequences=True, dropout=0.2, \n",
    "                                 recurrent_dropout=0.2)(lstm_branch)\n",
    "        lstm_branch = layers.LSTM(25, dropout=0.2, recurrent_dropout=0.2)(lstm_branch)\n",
    "        \n",
    "        # Attention mechanism for LSTM\n",
    "        attention = layers.Dense(25, activation='tanh')(lstm_branch)\n",
    "        attention = layers.Dense(1, activation='sigmoid')(attention)\n",
    "        lstm_weighted = layers.multiply([lstm_branch, attention])\n",
    "        \n",
    "        # Combine CNN and LSTM features\n",
    "        combined = layers.concatenate([cnn_branch, lstm_weighted])\n",
    "        \n",
    "        # Dense layers for final prediction\n",
    "        dense = layers.Dense(128, activation='relu')(combined)\n",
    "        dense = layers.Dropout(0.3)(dense)\n",
    "        dense = layers.Dense(64, activation='relu')(dense)\n",
    "        dense = layers.Dropout(0.2)(dense)\n",
    "        dense = layers.Dense(32, activation='relu')(dense)\n",
    "        \n",
    "        # Output layer - regression for 30-day returns\n",
    "        output = layers.Dense(1, activation='linear', name='return_prediction')(dense)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        \n",
    "        # Compile with custom loss function for financial data\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, df, validation_split=0.2, epochs=100, batch_size=32):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Engineer 30-day target\n",
    "        df_processed = self.engineer_30day_target(df)\n",
    "        print(f\"Data shape after 30-day target engineering: {df_processed.shape}\")\n",
    "        \n",
    "        # Prepare features\n",
    "        features, feature_names = self.prepare_features(df_processed)\n",
    "        targets = df_processed['target_return_30d'].values\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = self.create_sequences(features, targets)\n",
    "        print(f\"Sequence shape: X={X.shape}, y={y.shape}\")\n",
    "        \n",
    "        # Train/validation split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, shuffle=False  # No shuffle for time series\n",
    "        )\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history, X_val, y_val\n",
    "    \n",
    "    def evaluate_model(self, X_val, y_val):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        predictions = self.model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val, predictions)\n",
    "        mse = mean_squared_error(y_val, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_val, predictions)\n",
    "        \n",
    "        # Direction accuracy\n",
    "        direction_accuracy = np.mean(np.sign(y_val) == np.sign(predictions.flatten()))\n",
    "        \n",
    "        print(f\"\\nModel Performance:\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"R²: {r2:.6f}\")\n",
    "        print(f\"Direction Accuracy: {direction_accuracy:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "            'direction_accuracy': direction_accuracy\n",
    "        }\n",
    "    \n",
    "    def plot_predictions(self, X_val, y_val, sample_size=100):\n",
    "        \"\"\"Plot predictions vs actual\"\"\"\n",
    "        predictions = self.model.predict(X_val)\n",
    "        \n",
    "        # Sample for visualization\n",
    "        if len(y_val) > sample_size:\n",
    "            indices = np.random.choice(len(y_val), sample_size, replace=False)\n",
    "            y_sample = y_val[indices]\n",
    "            pred_sample = predictions[indices].flatten()\n",
    "        else:\n",
    "            y_sample = y_val\n",
    "            pred_sample = predictions.flatten()\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Time series plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(y_sample, label='Actual', alpha=0.7)\n",
    "        plt.plot(pred_sample, label='Predicted', alpha=0.7)\n",
    "        plt.title('30-Day Return Predictions vs Actual')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Return')\n",
    "        \n",
    "        # Scatter plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_val, predictions, alpha=0.6)\n",
    "        plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Returns')\n",
    "        plt.ylabel('Predicted Returns')\n",
    "        plt.title('Prediction Accuracy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict_next_30d(self, df, last_n_days=None):\n",
    "        \"\"\"Predict next 30-day return\"\"\"\n",
    "        if last_n_days is None:\n",
    "            last_n_days = self.sequence_length\n",
    "            \n",
    "        # Prepare features for the last sequence\n",
    "        features, _ = self.prepare_features(df)\n",
    "        last_sequence = features[-last_n_days:].reshape(1, last_n_days, features.shape[1])\n",
    "        \n",
    "        prediction = self.model.predict(last_sequence)\n",
    "        return prediction[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_all_data\n",
    "from sentiment import add_vader_sentiment, aggregate_daily_sentiment\n",
    "from feature_engineering import engineer_features\n",
    "\n",
    "btc_ohlcv, daily_oi, daily_funding_rate, df_news = load_all_data()\n",
    "# Assuming you have your df with engineered features\n",
    "df_news = add_vader_sentiment(df_news)\n",
    "df_newsdaily_sentiment = aggregate_daily_sentiment(df_news)\n",
    "\n",
    "# 3. Feature engineering\n",
    "df = engineer_features(btc_ohlcv, daily_oi, daily_funding_rate, df_newsdaily_sentiment)\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = BitcoinPredictor(sequence_length=60, prediction_horizon=30)\n",
    "\n",
    "# Train model\n",
    "history, X_val, y_val = predictor.train(df, epochs=100)\n",
    "\n",
    "# Evaluate\n",
    "metrics = predictor.evaluate_model(X_val, y_val)\n",
    "\n",
    "# Plot results\n",
    "predictor.plot_predictions(X_val, y_val)\n",
    "\n",
    "# Predict next 30 days\n",
    "next_30d_return = predictor.predict_next_30d(df)\n",
    "print(f\"Predicted 30-day return: {next_30d_return:.4f} ({next_30d_return*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedBitcoinPredictor:\n",
    "    def __init__(self, sequence_length=60, prediction_horizon=30, prune_gb=True, ridge_alpha=1.0):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.prune_gb = prune_gb\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.models = {}\n",
    "        self.meta_model = None\n",
    "        self.dir_model = None\n",
    "        self.scaler = None\n",
    "        self.regime_scaler = None\n",
    "        self.feature_groups = {\n",
    "            'price_volume': ['open', 'high', 'low', 'close', 'volume', 'high_close_ratio',\n",
    "                             'low_close_ratio', 'open_close_ratio', 'volume_avg_ratio', 'volume_change'],\n",
    "            'returns_momentum': ['returns_1d', 'returns_3d', 'returns_7d', 'log_returns',\n",
    "                                 'momentum_5', 'momentum_10'],\n",
    "            'technical': ['ma_5', 'price_ma_5_ratio', 'ma_10', 'price_ma_10_ratio', 'ma_20',\n",
    "                          'price_ma_20_ratio', 'ema_12', 'ema_26', 'macd', 'macd_signal',\n",
    "                          'macd_normalized', 'macd_signal_normalized', 'rsi', 'rsi_normalized'],\n",
    "            'volatility': ['bb_middle', 'bb_upper', 'bb_lower', 'bb_position', 'bb_width',\n",
    "                           'volatility_10', 'volatility_20'],\n",
    "            'sentiment': ['avg_vader_compound', 'article_count', 'vader_ma_3', 'vader_ma_7',\n",
    "                          'article_count_norm'],\n",
    "            'funding': ['funding_rate', 'funding_rate_ma'],\n",
    "            'temporal': ['day_sin', 'day_cos']\n",
    "        }\n",
    "    \n",
    "    def _ensure_numeric_series(self, series, column_name):\n",
    "        \"\"\"Safely convert series to numeric, handling mixed types\"\"\"\n",
    "        try:\n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "            # Fill NaN with 0 for calculations\n",
    "            return numeric_series.fillna(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert {column_name} to numeric: {e}\")\n",
    "            # Return a zero series of the same length\n",
    "            return pd.Series([0.0] * len(series), index=series.index)\n",
    "        \n",
    "    def detect_market_regimes(self, df):\n",
    "        \"\"\"Detect market regimes using clustering on market conditions\"\"\"\n",
    "        # Ensure consistent data types\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Features for regime detection\n",
    "        regime_features = [\n",
    "            'volatility_20', 'rsi', 'bb_position', 'returns_7d', \n",
    "            'volume_avg_ratio', 'funding_rate', 'avg_vader_compound'\n",
    "        ]\n",
    "        \n",
    "        available_regime_features = [f for f in regime_features if f in df.columns]\n",
    "        \n",
    "        if len(available_regime_features) < 4:\n",
    "            print(\"Warning: Not enough regime features available, using simple volatility-based regimes\")\n",
    "            return self._simple_volatility_regimes(df)\n",
    "        \n",
    "        # Ensure all regime features are numeric\n",
    "        for feature in available_regime_features:\n",
    "            df[feature] = self._ensure_numeric_series(df[feature], feature)\n",
    "        \n",
    "        # Scale regime features\n",
    "        self.regime_scaler = RobustScaler()\n",
    "        regime_data = self.regime_scaler.fit_transform(df[available_regime_features])\n",
    "        \n",
    "        # Use K-means to identify market regimes\n",
    "        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "        regimes = kmeans.fit_predict(regime_data)\n",
    "        \n",
    "        # Assign regime labels based on characteristics\n",
    "        regime_labels = []\n",
    "        for i in range(4):\n",
    "            regime_mask = regimes == i\n",
    "            if regime_mask.sum() > 0:  # Check if any samples in this regime\n",
    "                avg_vol = df.loc[regime_mask, 'volatility_20'].mean()\n",
    "                avg_returns = df.loc[regime_mask, 'returns_7d'].mean()\n",
    "                \n",
    "                if avg_vol > df['volatility_20'].quantile(0.75):\n",
    "                    if avg_returns > 0:\n",
    "                        label = 'bull_volatile'\n",
    "                    else:\n",
    "                        label = 'bear_volatile'\n",
    "                else:\n",
    "                    if avg_returns > 0:\n",
    "                        label = 'bull_stable'\n",
    "                    else:\n",
    "                        label = 'bear_stable'\n",
    "            else:\n",
    "                label = 'neutral'  # Default for empty regimes\n",
    "            \n",
    "            regime_labels.append(label)\n",
    "        \n",
    "        # Map regimes to labels\n",
    "        regime_mapping = {i: regime_labels[i] for i in range(4)}\n",
    "        labeled_regimes = [regime_mapping[r] for r in regimes]\n",
    "        \n",
    "        print(f\"Detected regimes distribution:\")\n",
    "        unique, counts = np.unique(labeled_regimes, return_counts=True)\n",
    "        for regime, count in zip(unique, counts):\n",
    "            print(f\"  {regime}: {count} days ({count/len(labeled_regimes)*100:.1f}%)\")\n",
    "        \n",
    "        return labeled_regimes\n",
    "    \n",
    "    def _simple_volatility_regimes(self, df):\n",
    "        \"\"\"Simple volatility-based regime detection as fallback\"\"\"\n",
    "        # Ensure numeric data\n",
    "        volatility = self._ensure_numeric_series(df['volatility_20'], 'volatility_20')\n",
    "        returns = self._ensure_numeric_series(df['returns_7d'], 'returns_7d')\n",
    "        \n",
    "        vol_25 = volatility.quantile(0.25)\n",
    "        vol_75 = volatility.quantile(0.75)\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(df)):\n",
    "            vol = volatility.iloc[i]\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            if vol > vol_75:\n",
    "                regime = 'bull_volatile' if ret > 0 else 'bear_volatile'\n",
    "            else:\n",
    "                regime = 'bull_stable' if ret > 0 else 'bear_stable'\n",
    "            \n",
    "            regimes.append(regime)\n",
    "        \n",
    "        return regimes\n",
    "    \n",
    "    def detect_extreme_conditions(self, df):\n",
    "        \"\"\"Detect extreme market conditions for special handling - FIXED VERSION\"\"\"\n",
    "        conditions = {}\n",
    "        \n",
    "        # Ensure all columns are numeric before operations\n",
    "        volatility_20 = self._ensure_numeric_series(df['volatility_20'], 'volatility_20')\n",
    "        returns_7d = self._ensure_numeric_series(df['returns_7d'], 'returns_7d')\n",
    "        \n",
    "        # Extreme volatility (top 10%)\n",
    "        vol_threshold = volatility_20.quantile(0.90)\n",
    "        conditions['extreme_vol'] = volatility_20 > vol_threshold\n",
    "        \n",
    "        # Extreme returns (beyond 2 standard deviations)\n",
    "        ret_std = returns_7d.std()\n",
    "        conditions['extreme_up'] = returns_7d > (2 * ret_std)\n",
    "        conditions['extreme_down'] = returns_7d < (-2 * ret_std)\n",
    "        \n",
    "        # Extreme funding rates\n",
    "        if 'funding_rate' in df.columns:\n",
    "            funding_rate = self._ensure_numeric_series(df['funding_rate'], 'funding_rate')\n",
    "            funding_std = funding_rate.std()\n",
    "            conditions['extreme_funding'] = np.abs(funding_rate) > (2 * funding_std)\n",
    "        else:\n",
    "            conditions['extreme_funding'] = pd.Series([False] * len(df), index=df.index)\n",
    "        \n",
    "        # Extreme sentiment\n",
    "        if 'avg_vader_compound' in df.columns:\n",
    "            sentiment = self._ensure_numeric_series(df['avg_vader_compound'], 'avg_vader_compound')\n",
    "            sent_std = sentiment.std()\n",
    "            conditions['extreme_sentiment'] = np.abs(sentiment) > (2 * sent_std)\n",
    "        else:\n",
    "            conditions['extreme_sentiment'] = pd.Series([False] * len(df), index=df.index)\n",
    "        \n",
    "        # Combine all extreme conditions safely\n",
    "        extreme_mask = (conditions['extreme_vol'] | \n",
    "                       conditions['extreme_up'] | \n",
    "                       conditions['extreme_down'] | \n",
    "                       conditions['extreme_funding'] | \n",
    "                       conditions['extreme_sentiment'])\n",
    "        \n",
    "        print(f\"Extreme conditions detected in {extreme_mask.sum()} days ({extreme_mask.sum()/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return extreme_mask, conditions\n",
    "    \n",
    "    def engineer_30day_target(self, df):\n",
    "        \"\"\"Engineer 30-day forward returns target with regime-aware adjustments - FIXED VERSION\"\"\"\n",
    "        df_target = df.copy()\n",
    "        \n",
    "        # Ensure index is DatetimeIndex\n",
    "        if not isinstance(df_target.index, pd.DatetimeIndex):\n",
    "            df_target.index = pd.to_datetime(df_target.index)\n",
    "        \n",
    "        # Ensure close prices are numeric\n",
    "        df_target['close'] = self._ensure_numeric_series(df_target['close'], 'close')\n",
    "        \n",
    "        # Basic 30-day return\n",
    "        df_target['target_return_30d'] = (df_target['close'].shift(-self.prediction_horizon) - \n",
    "                                         df_target['close']) / df_target['close']\n",
    "        \n",
    "        # Regime-adjusted targets (optional - can help with regime-specific training)\n",
    "        df_target['target_return_raw'] = df_target['target_return_30d'].copy()\n",
    "        \n",
    "        # Detect regimes and extreme conditions\n",
    "        regimes = self.detect_market_regimes(df_target)\n",
    "        extreme_mask, _ = self.detect_extreme_conditions(df_target)\n",
    "        \n",
    "        df_target['market_regime'] = regimes\n",
    "        df_target['extreme_condition'] = extreme_mask\n",
    "        \n",
    "        # Optional: Apply regime-specific target smoothing for extreme conditions\n",
    "        for regime in ['bull_volatile', 'bear_volatile']:\n",
    "            # Create boolean mask safely\n",
    "            regime_condition = pd.Series(regimes) == regime\n",
    "            regime_mask = regime_condition & extreme_mask\n",
    "            \n",
    "            if regime_mask.sum() > 0:\n",
    "                # Apply slight smoothing to extreme targets to prevent overfitting\n",
    "                smoothed_values = (\n",
    "                    df_target.loc[regime_mask, 'target_return_30d'] * 0.8 + \n",
    "                    df_target.loc[regime_mask, 'target_return_30d'].rolling(5, center=True).mean().fillna(0) * 0.2\n",
    "                )\n",
    "                df_target.loc[regime_mask, 'target_return_30d'] = smoothed_values\n",
    "        \n",
    "        df_target['target_direction_30d'] = (df_target['target_return_30d'] > 0).astype(int)\n",
    "        df_target = df_target.dropna()\n",
    "        \n",
    "        return df_target\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Enhanced feature preparation with regime-aware scaling\"\"\"\n",
    "        # Get all feature columns\n",
    "        feature_cols = []\n",
    "        for group_features in self.feature_groups.values():\n",
    "            feature_cols.extend(group_features)\n",
    "        \n",
    "        available_features = [col for col in feature_cols if col in df.columns]\n",
    "        \n",
    "        # Ensure all feature columns are numeric\n",
    "        for col in available_features:\n",
    "            if col in df.columns:\n",
    "                df[col] = self._ensure_numeric_series(df[col], col)\n",
    "        \n",
    "        # Add regime and extreme condition features\n",
    "        if 'market_regime' in df.columns:\n",
    "            # One-hot encode regimes\n",
    "            regime_dummies = pd.get_dummies(df['market_regime'], prefix='regime')\n",
    "            for col in regime_dummies.columns:\n",
    "                df[col] = regime_dummies[col].astype(float)  # Ensure numeric\n",
    "                available_features.append(col)\n",
    "        \n",
    "        if 'extreme_condition' in df.columns:\n",
    "            df['extreme_condition'] = df['extreme_condition'].astype(float)  # Convert bool to float\n",
    "            available_features.append('extreme_condition')\n",
    "        \n",
    "        # Add additional engineered features for extreme conditions\n",
    "        additional_features = [col for col in df.columns if col not in available_features \n",
    "                             and col not in ['target_return_30d', 'target_direction_30d', \n",
    "                                           'target_return_raw', 'market_regime', 'next_close', \n",
    "                                           'target_return', 'target_direction']]\n",
    "        \n",
    "        # Ensure additional features are numeric\n",
    "        for col in additional_features:\n",
    "            if col in df.columns:\n",
    "                df[col] = self._ensure_numeric_series(df[col], col)\n",
    "        \n",
    "        all_features = available_features + additional_features\n",
    "        \n",
    "        print(f\"Using {len(all_features)} features for ensemble training\")\n",
    "        \n",
    "        # Robust scaling with outlier handling\n",
    "        self.scaler = RobustScaler(quantile_range=(5, 95))  # More aggressive outlier handling\n",
    "        scaled_features = self.scaler.fit_transform(df[all_features])\n",
    "        \n",
    "        return scaled_features, all_features\n",
    "    \n",
    "    def build_cnn_lstm_model(self, input_shape, regime_specific=False):\n",
    "        \"\"\"Enhanced CNN-LSTM with attention and dropout for extreme conditions\"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Enhanced CNN branch with proper residual connections\n",
    "        # First, project input to match CNN output dimensions\n",
    "        input_projection = layers.Conv1D(filters=128, kernel_size=1, activation='linear', padding='same')(inputs)\n",
    "        \n",
    "        cnn_branch = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "        cnn_branch = layers.BatchNormalization()(cnn_branch)\n",
    "        cnn_branch = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(cnn_branch)\n",
    "        \n",
    "        # Now we can add residual connection (both are 128 filters)\n",
    "        cnn_residual = layers.Add()([input_projection, cnn_branch])\n",
    "        \n",
    "        cnn_branch = layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(cnn_residual)\n",
    "        cnn_branch = layers.BatchNormalization()(cnn_branch)\n",
    "        cnn_branch = layers.MaxPooling1D(pool_size=2)(cnn_branch)\n",
    "        cnn_branch = layers.Dropout(0.3)(cnn_branch)\n",
    "        \n",
    "        cnn_branch = layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(cnn_branch)\n",
    "        cnn_branch = layers.GlobalMaxPooling1D()(cnn_branch)\n",
    "        \n",
    "        # Enhanced LSTM branch with bidirectional processing\n",
    "        lstm_branch = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.3, \n",
    "                                                      recurrent_dropout=0.3))(inputs)\n",
    "        lstm_branch = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.3, \n",
    "                                                      recurrent_dropout=0.3))(lstm_branch)\n",
    "        lstm_branch = layers.Bidirectional(layers.LSTM(32, dropout=0.3, recurrent_dropout=0.3))(lstm_branch)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        attention = layers.Dense(64, activation='tanh')(lstm_branch)\n",
    "        attention = layers.Dense(32, activation='tanh')(attention)\n",
    "        attention = layers.Dense(1, activation='sigmoid')(attention)\n",
    "        lstm_weighted = layers.multiply([lstm_branch, attention])\n",
    "        \n",
    "        # Combine features\n",
    "        combined = layers.concatenate([cnn_branch, lstm_weighted])\n",
    "        \n",
    "        # Enhanced dense layers with adaptive dropout\n",
    "        dense = layers.Dense(256, activation='relu')(combined)\n",
    "        dense = layers.Dropout(0.4)(dense)\n",
    "        dense = layers.Dense(128, activation='relu')(dense)\n",
    "        dense = layers.Dropout(0.3)(dense)\n",
    "        dense = layers.Dense(64, activation='relu')(dense)\n",
    "        dense = layers.Dropout(0.2)(dense)\n",
    "        \n",
    "        # Output layer\n",
    "        output = layers.Dense(1, activation='linear', name='return_prediction')(dense)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        \n",
    "        # Compile with Huber loss (more robust to outliers)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "            loss=tf.keras.losses.Huber(delta=0.1),  # Robust to outliers\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_sequences(self, features, targets, regimes=None):\n",
    "        \"\"\"Create sequences with regime information\"\"\"\n",
    "        X, y, regime_seq = [], [], []\n",
    "        \n",
    "        for i in range(len(features) - self.sequence_length - self.prediction_horizon + 1):\n",
    "            X.append(features[i:(i + self.sequence_length)])\n",
    "            y.append(targets[i + self.sequence_length])\n",
    "            if regimes is not None:\n",
    "                regime_seq.append(regimes[i + self.sequence_length])\n",
    "        \n",
    "        return np.array(X), np.array(y), regime_seq\n",
    "    \n",
    "    def train_ensemble(self, df, validation_split=0.2, epochs=150, batch_size=32):\n",
    "        # Data type safety check at the beginning\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        df_proc = self.engineer_30day_target(df)\n",
    "        features, _ = self.prepare_features(df_proc)\n",
    "        targets = df_proc['target_return_30d'].values\n",
    "        regimes = df_proc['market_regime'].values\n",
    "\n",
    "        X, y, regime_seq = self.create_sequences(features, targets, regimes)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, shuffle=False)\n",
    "        X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "        X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "\n",
    "        # Base models\n",
    "        self.models['cnn_lstm'] = self.build_cnn_lstm_model((X.shape[1], X.shape[2]))\n",
    "        es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "        self.models['cnn_lstm'].fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                                    epochs=epochs, batch_size=batch_size,\n",
    "                                    callbacks=[es, rl], verbose=1)\n",
    "\n",
    "        self.models['random_forest'] = RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "        self.models['random_forest'].fit(X_train_flat, y_train)\n",
    "\n",
    "        if not self.prune_gb:\n",
    "            self.models['gradient_boosting'] = GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42)\n",
    "            self.models['gradient_boosting'].fit(X_train_flat, y_train)\n",
    "\n",
    "        # Stacking\n",
    "        preds = []\n",
    "        names = ['cnn_lstm', 'random_forest'] + (['gradient_boosting'] if not self.prune_gb else [])\n",
    "        for name in names:\n",
    "            if name == 'cnn_lstm':\n",
    "                preds.append(self.models[name].predict(X_val).flatten())\n",
    "            else:\n",
    "                preds.append(self.models[name].predict(X_val_flat))\n",
    "        stacked = np.vstack(preds).T  # shape (n_samples, n_models)\n",
    "\n",
    "        # Ridge meta-learner with non-negative coefficients\n",
    "        self.meta_model = Ridge(alpha=self.ridge_alpha, positive=True)\n",
    "        self.meta_model.fit(stacked, y_val)\n",
    "        print(\"Meta-learner coefs:\", self.meta_model.coef_)\n",
    "        return X_val, y_val, regime_seq\n",
    "\n",
    "    def predict_ensemble(self, X):\n",
    "        \"\"\"Make ensemble predictions and also return individual model outputs and meta weights.\"\"\"\n",
    "        # Compute individual predictions\n",
    "        individual_preds = {}\n",
    "        X_flat = X.reshape(len(X), -1)\n",
    "        names = ['cnn_lstm', 'random_forest'] + (['gradient_boosting'] if 'gradient_boosting' in self.models else [])\n",
    "        for name in names:\n",
    "            if name == 'cnn_lstm':\n",
    "                pred = self.models[name].predict(X).flatten()\n",
    "            else:\n",
    "                pred = self.models[name].predict(X_flat)\n",
    "            individual_preds[name] = pred\n",
    "\n",
    "        # Stack for meta-model\n",
    "        stacked = np.vstack([individual_preds[name] for name in names]).T\n",
    "        ensemble = self.meta_model.predict(stacked)\n",
    "\n",
    "        # Meta-model weights for interpretability\n",
    "        weights = {'meta_coefs': self.meta_model.coef_}\n",
    "        return ensemble.reshape(-1,1), individual_preds, weights\n",
    "\n",
    "    def evaluate_ensemble(self, X_val, y_val, regime_seq_val=None):\n",
    "        \"\"\"Evaluate ensemble performance with provided validation set.\"\"\"\n",
    "        ensemble_pred, individual_preds, weights = self.predict_ensemble(X_val)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val, ensemble_pred)\n",
    "        mse = mean_squared_error(y_val, ensemble_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_val, ensemble_pred)\n",
    "        direction_accuracy = np.mean(np.sign(y_val) == np.sign(ensemble_pred.flatten()))\n",
    "\n",
    "        print(f\"\\n=== Ensemble Performance ===\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"R²: {r2:.6f}\")\n",
    "        print(f\"Direction Accuracy: {direction_accuracy:.4f}\")\n",
    "\n",
    "        print(f\"\\n=== Individual Model Performance ===\")\n",
    "        for model_name, pred in individual_preds.items():\n",
    "            model_mae = mean_absolute_error(y_val, pred)\n",
    "            model_mse = mean_squared_error(y_val, pred)\n",
    "            model_rmse = np.sqrt(mse)\n",
    "            model_r2 = r2_score(y_val, pred)\n",
    "            model_dir_acc = np.mean(np.sign(y_val) == np.sign(pred.flatten()))\n",
    "            print(f\"{model_name}: MAE={model_mae:.6f}, MSE={model_mse:.6f}, RMSE={model_rmse:.6f}, R²={model_r2:.6f}, Dir_Acc={model_dir_acc:.4f}\")\n",
    "\n",
    "        print(f\"\\nMeta-model weights: {weights['meta_coefs']}\")\n",
    "        return {\n",
    "            'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "            'direction_accuracy': direction_accuracy,\n",
    "            'individual_performance': individual_preds,\n",
    "            'meta_weights': weights['meta_coefs']\n",
    "        }\n",
    "\n",
    "    def validate_meta_learner(self, df, n_splits=5, stratify=False, epochs=150, batch_size=32):\n",
    "        \"\"\"\n",
    "        Perform rolling-window cross-validation to assess stability of meta-learner coefficients.\n",
    "        Prints per-fold and mean coefficients for stacking meta-model.\n",
    "        \"\"\"\n",
    "        # Data type safety check\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "        # Prepare processed data\n",
    "        df_proc = self.engineer_30day_target(df)\n",
    "        features, _ = self.prepare_features(df_proc)\n",
    "        targets = df_proc['target_return_30d'].values\n",
    "        regimes = df_proc['market_regime'].values\n",
    "        X, y, regime_seq = self.create_sequences(features, targets, regimes)\n",
    "\n",
    "        # Select splitter\n",
    "        if stratify:\n",
    "            splitter = StratifiedKFold(n_splits=n_splits, shuffle=False)\n",
    "            split_fn = lambda: splitter.split(X, regime_seq)\n",
    "        else:\n",
    "            splitter = TimeSeriesSplit(n_splits=n_splits)\n",
    "            split_fn = lambda: splitter.split(X)\n",
    "\n",
    "        coefs = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(split_fn()):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "            X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "\n",
    "            # Retrain base models\n",
    "            m1 = self.build_cnn_lstm_model((X.shape[1], X.shape[2]))\n",
    "            m1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs,\n",
    "                   batch_size=batch_size, callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=5)], verbose=0)\n",
    "            m2 = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "            m2.fit(X_train_flat, y_train)\n",
    "            models = [m1, m2]\n",
    "            if not self.prune_gb:\n",
    "                m3 = GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42)\n",
    "                m3.fit(X_train_flat, y_train)\n",
    "                models.append(m3)\n",
    "\n",
    "            # Stack predictions\n",
    "            preds = []\n",
    "            for i, mdl in enumerate(models):\n",
    "                if i == 0:\n",
    "                    preds.append(mdl.predict(X_val).flatten())\n",
    "                else:\n",
    "                    preds.append(mdl.predict(X_val_flat))\n",
    "            stacked = np.vstack(preds).T\n",
    "\n",
    "            # Fit Ridge meta-model\n",
    "            meta = Ridge(alpha=self.ridge_alpha, positive=True)\n",
    "            meta.fit(stacked, y_val)\n",
    "            print(f\"Fold {fold+1} coefs: {meta.coef_}\")\n",
    "            coefs.append(meta.coef_)\n",
    "\n",
    "        coefs = np.array(coefs)\n",
    "        print(\"Mean coefs:\", coefs.mean(axis=0), \"Std dev:\", coefs.std(axis=0))\n",
    "        return coefs\n",
    "            \n",
    "    def build_direction_model(self, input_shape):\n",
    "        \"\"\"Build a Bidirectional LSTM for binary direction classification.\"\"\"\n",
    "        inp = layers.Input(shape=input_shape)\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2))(inp)\n",
    "        x = layers.Bidirectional(layers.LSTM(32, dropout=0.2))(x)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        out = layers.Dense(1, activation='sigmoid', name='direction')(x)\n",
    "        model = Model(inputs=inp, outputs=out)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train_direction_classifier(self, df, validation_split=0.2, epochs=50, batch_size=32):\n",
    "        \"\"\"Train the direction classifier using 30-day direction labels.\"\"\"\n",
    "        # Data type safety check\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "        df_proc = self.engineer_30day_target(df)\n",
    "        features, _ = self.prepare_features(df_proc)\n",
    "        targets = df_proc['target_direction_30d'].values\n",
    "        X, y, _ = self.create_sequences(features, targets)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, shuffle=False)\n",
    "\n",
    "        self.dir_model = self.build_direction_model((X.shape[1], X.shape[2]))\n",
    "        es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "        history = self.dir_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, rl],\n",
    "            verbose=1\n",
    "        )\n",
    "        return history, X_val, y_val\n",
    "\n",
    "    def predict_direction(self, X):\n",
    "        \"\"\"Predict direction probabilities (0–1) with the trained classifier.\"\"\"\n",
    "        if self.dir_model is None:\n",
    "            raise ValueError(\"Direction model not trained. Call train_direction_classifier first.\")\n",
    "        return self.dir_model.predict(X, verbose=0).flatten()\n",
    "\n",
    "    def predict_next_30d(self, df, threshold=0.5):\n",
    "        \"\"\"Predict next 30-day direction & return, gating regression by classifier.\"\"\"\n",
    "        # Data type safety check\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "        # Prepare regression inputs\n",
    "        features, _ = self.prepare_features(df)\n",
    "        seq = features[-self.sequence_length:].reshape(1, self.sequence_length, -1)\n",
    "\n",
    "        # Get direction probability\n",
    "        dir_prob = self.predict_direction(seq)\n",
    "\n",
    "        # Get regression prediction\n",
    "        reg_pred, _, _ = self.predict_ensemble(seq)\n",
    "        ret_pred = reg_pred[0][0]\n",
    "\n",
    "        # Gate output based on direction confidence\n",
    "        direction = 1 if dir_prob >= threshold else 0\n",
    "        gated_return = ret_pred if direction == 1 else -ret_pred\n",
    "\n",
    "        return {'direction_prob': dir_prob[0],\n",
    "                'predicted_direction': direction,\n",
    "                'predicted_return': gated_return}\n",
    "    \n",
    "    def evaluate_direction_classifier(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Evaluate the trained direction classifier on validation data.\n",
    "        Computes accuracy, precision, recall, F1-score, and returns a report.\n",
    "        \"\"\"\n",
    "        if self.dir_model is None:\n",
    "            raise ValueError(\"Direction model not trained. Call train_direction_classifier first.\")\n",
    "        # Predict probabilities and labels\n",
    "        probs = self.dir_model.predict(X_val, verbose=0).flatten()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "        acc = accuracy_score(y_val, preds)\n",
    "        prec = precision_score(y_val, preds)\n",
    "        rec = recall_score(y_val, preds)\n",
    "        f1 = f1_score(y_val, preds)\n",
    "        auc = roc_auc_score(y_val, probs)\n",
    "        cm = confusion_matrix(y_val, preds)\n",
    "\n",
    "        # Print report\n",
    "        print(\"=== Direction Classifier Evaluation ===\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Precision: {prec:.4f}\")\n",
    "        print(f\"Recall: {rec:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {auc:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': auc,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "    \n",
    "    def train_and_evaluate_consistently(self, df, validation_split=0.2, epochs=150, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train both ensemble and direction classifier on the same data splits\n",
    "        for consistent evaluation and comparison.\n",
    "        \"\"\"\n",
    "        # Data type safety check\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        # Single data preparation\n",
    "        df_proc = self.engineer_30day_target(df)\n",
    "        features, feature_names = self.prepare_features(df_proc)\n",
    "        \n",
    "        # Get both targets\n",
    "        return_targets = df_proc['target_return_30d'].values\n",
    "        direction_targets = df_proc['target_direction_30d'].values\n",
    "        regimes = df_proc['market_regime'].values\n",
    "        \n",
    "        # Create sequences for both tasks\n",
    "        X_return, y_return, regime_seq = self.create_sequences(features, return_targets, regimes)\n",
    "        X_direction, y_direction, _ = self.create_sequences(features, direction_targets, regimes)\n",
    "        \n",
    "        # Use the SAME split for both tasks\n",
    "        train_size = int(len(X_return) * (1 - validation_split))\n",
    "        \n",
    "        # Split data consistently\n",
    "        X_train = X_return[:train_size]\n",
    "        X_val = X_return[train_size:]\n",
    "        y_return_train = y_return[:train_size]\n",
    "        y_return_val = y_return[train_size:]\n",
    "        y_direction_train = y_direction[:train_size]\n",
    "        y_direction_val = y_direction[train_size:]\n",
    "        regime_seq_train = regime_seq[:train_size]\n",
    "        regime_seq_val = regime_seq[train_size:]\n",
    "        \n",
    "        print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples\")\n",
    "        \n",
    "        # Train ensemble (regression)\n",
    "        X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "        X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "        \n",
    "        # Base models\n",
    "        self.models['cnn_lstm'] = self.build_cnn_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "        es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "        \n",
    "        print(\"Training CNN-LSTM...\")\n",
    "        self.models['cnn_lstm'].fit(X_train, y_return_train, \n",
    "                                validation_data=(X_val, y_return_val),\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                callbacks=[es, rl], verbose=1)\n",
    "        \n",
    "        print(\"Training Random Forest...\")\n",
    "        self.models['random_forest'] = RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "        self.models['random_forest'].fit(X_train_flat, y_return_train)\n",
    "        \n",
    "        if not self.prune_gb:\n",
    "            print(\"Training Gradient Boosting...\")\n",
    "            self.models['gradient_boosting'] = GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42)\n",
    "            self.models['gradient_boosting'].fit(X_train_flat, y_return_train)\n",
    "        \n",
    "        # Train meta-learner\n",
    "        preds = []\n",
    "        names = ['cnn_lstm', 'random_forest'] + (['gradient_boosting'] if not self.prune_gb else [])\n",
    "        for name in names:\n",
    "            if name == 'cnn_lstm':\n",
    "                preds.append(self.models[name].predict(X_val).flatten())\n",
    "            else:\n",
    "                preds.append(self.models[name].predict(X_val_flat))\n",
    "        stacked = np.vstack(preds).T\n",
    "        \n",
    "        self.meta_model = Ridge(alpha=self.ridge_alpha, positive=True)\n",
    "        self.meta_model.fit(stacked, y_return_val)\n",
    "        print(\"Meta-learner coefficients:\", self.meta_model.coef_)\n",
    "        \n",
    "        # Train direction classifier\n",
    "        print(\"Training direction classifier...\")\n",
    "        self.dir_model = self.build_direction_model((X_train.shape[1], X_train.shape[2]))\n",
    "        es_dir = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        rl_dir = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "        \n",
    "        self.dir_model.fit(X_train, y_direction_train,\n",
    "                        validation_data=(X_val, y_direction_val),\n",
    "                        epochs=50, batch_size=batch_size,\n",
    "                        callbacks=[es_dir, rl_dir], verbose=1)\n",
    "        \n",
    "        return X_val, y_return_val, y_direction_val, regime_seq_val\n",
    "\n",
    "    def evaluate_both_consistently(self, X_val, y_return_val, y_direction_val, regime_seq_val=None):\n",
    "        \"\"\"\n",
    "        Evaluate both ensemble and direction classifier on the same validation data\n",
    "        for direct comparison.\n",
    "        \"\"\"\n",
    "        # Get ensemble predictions\n",
    "        ensemble_pred, individual_preds, weights = self.predict_ensemble(X_val)\n",
    "        \n",
    "        # Get direction classifier predictions\n",
    "        direction_probs = self.predict_direction(X_val)\n",
    "        direction_preds = (direction_probs >= 0.5).astype(int)\n",
    "        \n",
    "        # Evaluate regression ensemble\n",
    "        mae = mean_absolute_error(y_return_val, ensemble_pred)\n",
    "        mse = mean_squared_error(y_return_val, ensemble_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_return_val, ensemble_pred)\n",
    "        \n",
    "        # Direction accuracy from ensemble (regression → direction)\n",
    "        ensemble_direction_accuracy = np.mean(np.sign(y_return_val) == np.sign(ensemble_pred.flatten()))\n",
    "        \n",
    "        # Direction classifier metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        dir_accuracy = accuracy_score(y_direction_val, direction_preds)\n",
    "        dir_precision = precision_score(y_direction_val, direction_preds)\n",
    "        dir_recall = recall_score(y_direction_val, direction_preds)\n",
    "        dir_f1 = f1_score(y_direction_val, direction_preds)\n",
    "        dir_auc = roc_auc_score(y_direction_val, direction_probs)\n",
    "        \n",
    "        print(f\"\\n=== CONSISTENT EVALUATION ON SAME VALIDATION SET ===\")\n",
    "        print(f\"Validation set size: {len(X_val)} samples\")\n",
    "        print(f\"Validation period: Last {len(X_val)} sequences\")\n",
    "        \n",
    "        print(f\"\\n=== Ensemble (Regression) Performance ===\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"R²: {r2:.6f}\")\n",
    "        print(f\"Direction Accuracy (from regression): {ensemble_direction_accuracy:.4f}\")\n",
    "        \n",
    "        print(f\"\\n=== Direction Classifier Performance ===\")\n",
    "        print(f\"Binary Classification Accuracy: {dir_accuracy:.4f}\")\n",
    "        print(f\"Precision: {dir_precision:.4f}\")\n",
    "        print(f\"Recall: {dir_recall:.4f}\")\n",
    "        print(f\"F1-score: {dir_f1:.4f}\")\n",
    "        print(f\"ROC AUC: {dir_auc:.4f}\")\n",
    "        \n",
    "        print(f\"\\n=== COMPARISON ===\")\n",
    "        print(f\"Ensemble direction accuracy: {ensemble_direction_accuracy:.4f}\")\n",
    "        print(f\"Dedicated classifier accuracy: {dir_accuracy:.4f}\")\n",
    "        print(f\"Improvement from dedicated classifier: {dir_accuracy - ensemble_direction_accuracy:.4f}\")\n",
    "        \n",
    "        # Regime-specific analysis if available\n",
    "        if regime_seq_val is not None:\n",
    "            print(f\"\\n=== Regime-Specific Performance ===\")\n",
    "            for regime in np.unique(regime_seq_val):\n",
    "                mask = np.array(regime_seq_val) == regime\n",
    "                if mask.sum() > 0:\n",
    "                    regime_ensemble_acc = np.mean(np.sign(y_return_val[mask]) == np.sign(ensemble_pred.flatten()[mask]))\n",
    "                    regime_dir_acc = accuracy_score(y_direction_val[mask], direction_preds[mask])\n",
    "                    print(f\"{regime}: Ensemble={regime_ensemble_acc:.4f}, Classifier={regime_dir_acc:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'ensemble': {\n",
    "                'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "                'direction_accuracy': ensemble_direction_accuracy\n",
    "            },\n",
    "            'direction_classifier': {\n",
    "                'accuracy': dir_accuracy, 'precision': dir_precision, \n",
    "                'recall': dir_recall, 'f1': dir_f1, 'auc': dir_auc\n",
    "            },\n",
    "            'meta_weights': weights['meta_coefs']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_all_data\n",
    "from sentiment import add_vader_sentiment, aggregate_daily_sentiment\n",
    "from feature_engineering import engineer_features\n",
    "\n",
    "btc_ohlcv, daily_oi, daily_funding_rate, df_news = load_all_data()\n",
    "# Assuming you have your df with engineered features\n",
    "df_news = add_vader_sentiment(df_news)\n",
    "df_newsdaily_sentiment = aggregate_daily_sentiment(df_news)\n",
    "\n",
    "# 3. Feature engineering\n",
    "df = engineer_features(btc_ohlcv, daily_oi, daily_funding_rate, df_newsdaily_sentiment)\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = AdvancedBitcoinPredictor(sequence_length=60, prediction_horizon=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected regimes distribution:\n",
      "  bear_stable: 1148 days (40.0%)\n",
      "  bear_volatile: 314 days (11.0%)\n",
      "  bull_stable: 1040 days (36.3%)\n",
      "  bull_volatile: 365 days (12.7%)\n",
      "Extreme conditions detected in 403 days (14.1%)\n",
      "Using 46 features for ensemble training\n",
      "Epoch 1/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - loss: 0.0883 - mae: 0.9308 - mse: 3.8792 - val_loss: 0.0099 - val_mae: 0.1401 - val_mse: 0.0356 - learning_rate: 5.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - loss: 0.0207 - mae: 0.2518 - mse: 0.1306 - val_loss: 0.0082 - val_mae: 0.1223 - val_mse: 0.0282 - learning_rate: 5.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - loss: 0.0163 - mae: 0.2074 - mse: 0.0827 - val_loss: 0.0089 - val_mae: 0.1294 - val_mse: 0.0312 - learning_rate: 5.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - loss: 0.0156 - mae: 0.2003 - mse: 0.0729 - val_loss: 0.0091 - val_mae: 0.1322 - val_mse: 0.0325 - learning_rate: 5.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - loss: 0.0145 - mae: 0.1892 - mse: 0.0661 - val_loss: 0.0100 - val_mae: 0.1411 - val_mse: 0.0360 - learning_rate: 5.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 100ms/step - loss: 0.0138 - mae: 0.1816 - mse: 0.0613 - val_loss: 0.0089 - val_mae: 0.1302 - val_mse: 0.0312 - learning_rate: 5.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - loss: 0.0130 - mae: 0.1730 - mse: 0.0575 - val_loss: 0.0090 - val_mae: 0.1312 - val_mse: 0.0325 - learning_rate: 5.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - loss: 0.0126 - mae: 0.1683 - mse: 0.0541 - val_loss: 0.0096 - val_mae: 0.1373 - val_mse: 0.0352 - learning_rate: 2.5000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - loss: 0.0116 - mae: 0.1586 - mse: 0.0494 - val_loss: 0.0110 - val_mae: 0.1516 - val_mse: 0.0413 - learning_rate: 2.5000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - loss: 0.0114 - mae: 0.1561 - mse: 0.0480 - val_loss: 0.0107 - val_mae: 0.1487 - val_mse: 0.0390 - learning_rate: 2.5000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - loss: 0.0107 - mae: 0.1488 - mse: 0.0435 - val_loss: 0.0116 - val_mae: 0.1591 - val_mse: 0.0433 - learning_rate: 2.5000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 92ms/step - loss: 0.0106 - mae: 0.1482 - mse: 0.0430 - val_loss: 0.0119 - val_mae: 0.1626 - val_mse: 0.0457 - learning_rate: 2.5000e-04\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step\n",
      "Meta-learner coefs: [0.30009652 0.        ]\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "=== Ensemble Performance ===\n",
      "MAE: 0.118469\n",
      "MSE: 0.023295\n",
      "RMSE: 0.152626\n",
      "R²: 0.015172\n",
      "Direction Accuracy: 0.6291\n",
      "\n",
      "=== Individual Model Performance ===\n",
      "cnn_lstm: MAE=0.122257, MSE=0.028184, RMSE=0.152626, R²=-0.191532, Dir_Acc=0.5800\n",
      "random_forest: MAE=0.175118, MSE=0.053155, RMSE=0.152626, R²=-1.247246, Dir_Acc=0.3782\n",
      "\n",
      "Meta-model weights: [0.30009652 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Train ensemble\n",
    "X_val, y_val, regime_seq = predictor.train_ensemble(df, epochs=150)\n",
    "# Evaluate ensemble\n",
    "metrics = predictor.evaluate_ensemble(X_val, y_val, regime_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected regimes distribution:\n",
      "  bear_stable: 1148 days (40.0%)\n",
      "  bear_volatile: 314 days (11.0%)\n",
      "  bull_stable: 1040 days (36.3%)\n",
      "  bull_volatile: 365 days (12.7%)\n",
      "Extreme conditions detected in 403 days (14.1%)\n",
      "Using 46 features for ensemble training\n",
      "Epoch 1/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5486 - loss: 0.6765 - val_accuracy: 0.4018 - val_loss: 1.3451 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6981 - loss: 0.5847 - val_accuracy: 0.3909 - val_loss: 1.7195 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7809 - loss: 0.5019 - val_accuracy: 0.4091 - val_loss: 2.1406 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8201 - loss: 0.4144 - val_accuracy: 0.4127 - val_loss: 2.5425 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8630 - loss: 0.3423 - val_accuracy: 0.4727 - val_loss: 2.1652 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8704 - loss: 0.3050 - val_accuracy: 0.4455 - val_loss: 1.9813 - learning_rate: 5.0000e-04\n",
      "=== Direction Classifier Evaluation ===\n",
      "Accuracy: 0.4018\n",
      "Precision: 0.6349\n",
      "Recall: 0.1156\n",
      "F1-score: 0.1956\n",
      "ROC AUC: 0.5855\n",
      "Confusion Matrix:\n",
      "[[181  23]\n",
      " [306  40]]\n"
     ]
    }
   ],
   "source": [
    "history, X_val, y_val = predictor.train_direction_classifier(df)\n",
    "metrics = predictor.evaluate_direction_classifier(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = predictor.validate_meta_learner(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_hybrid(predictor, df, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Hybrid backtest: use direction classifier as gate on regression forecasts.\n",
    "    Returns a DataFrame and performance metrics.\n",
    "    \"\"\"\n",
    "    # 1. Prepare data\n",
    "    df_p = predictor.engineer_30day_target(df)\n",
    "    features, _ = predictor.prepare_features(df_p)\n",
    "    X_seq, _, _ = predictor.create_sequences(\n",
    "        features,\n",
    "        df_p['target_direction_30d'].values\n",
    "    )\n",
    "\n",
    "    # 2. Get model outputs\n",
    "    probs = predictor.predict_direction(X_seq)              # shape (n,)\n",
    "    reg_preds_2d, _, _ = predictor.predict_ensemble(X_seq)   # shape (n,1)\n",
    "    reg_preds = reg_preds_2d.flatten()                      # now shape (n,)\n",
    "\n",
    "    # 3. Build strategy returns (long/short/flat)\n",
    "    strat_return = np.where(\n",
    "        probs >= threshold,\n",
    "        reg_preds,\n",
    "        np.where(probs <= (1 - threshold),\n",
    "                 -reg_preds,\n",
    "                 0.0)\n",
    "    )\n",
    "\n",
    "    # 4. Assemble results (all 1-D arrays)\n",
    "    results = pd.DataFrame({\n",
    "        'direction_prob': probs,\n",
    "        'predicted_return': reg_preds,\n",
    "        'strategy_return': strat_return\n",
    "    })\n",
    "\n",
    "    # 5. Cumulative P&L\n",
    "    results['cum_strategy'] = (1 + results['strategy_return']).cumprod() - 1\n",
    "\n",
    "    # 6. Metrics\n",
    "    sharpe = (\n",
    "        results['strategy_return'].mean() /\n",
    "        results['strategy_return'].std()\n",
    "    ) * np.sqrt(252)\n",
    "    total = results['cum_strategy'].iloc[-1]\n",
    "    drawdown = (\n",
    "        results['cum_strategy'].cummax() -\n",
    "        results['cum_strategy']\n",
    "    ).max()\n",
    "\n",
    "    perf = {\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'max_drawdown': drawdown,\n",
    "        'total_return': total\n",
    "    }\n",
    "    return results, perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.train_direction_classifier(df)\n",
    "predictor.train_ensemble(df)\n",
    "results_df, perf_metrics = backtest_hybrid(predictor, df, threshold=0.7)\n",
    "print(perf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
