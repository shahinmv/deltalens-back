{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617b080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, random\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras import layers, callbacks, Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from feature_engineering import engineer_features\n",
    "from data_loader import load_all_data\n",
    "from sentiment import add_vader_sentiment, aggregate_daily_sentiment\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92d7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegimeAwareBitcoinPredictor:\n",
    "    \"\"\"\n",
    "    Bitcoin predictor with advanced regime detection specifically designed\n",
    "    to handle the July 2023 - January 2024 bear market period\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=60, prediction_horizon=30, \n",
    "                 max_position_size=0.20, stop_loss_threshold=0.10,\n",
    "                 bear_market_threshold=-0.15, prune_gb=True, ridge_alpha=2.0):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.max_position_size = max_position_size\n",
    "        self.stop_loss_threshold = stop_loss_threshold\n",
    "        self.bear_market_threshold = bear_market_threshold\n",
    "        self.prune_gb = prune_gb\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        \n",
    "        # Model components\n",
    "        self.models = {}\n",
    "        self.regime_specific_models = {}\n",
    "        self.meta_model = None\n",
    "        self.scaler = None\n",
    "        self.regime_scaler = None\n",
    "        self.trained_feature_count = None\n",
    "        self.expected_regime_columns = None\n",
    "        \n",
    "        # Regime tracking\n",
    "        self.current_regime = 'neutral'\n",
    "        self.regime_history = []\n",
    "        self.bear_market_detected = False\n",
    "        self.trend_momentum = 0.0\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.prediction_history = []\n",
    "        self.consecutive_losses = 0\n",
    "        \n",
    "        # Feature groups - simplified for better generalization\n",
    "        self.feature_groups = {\n",
    "            'price_volume': ['open', 'high', 'low', 'close', 'volume', 'high_close_ratio',\n",
    "                             'low_close_ratio', 'open_close_ratio', 'volume_avg_ratio'],\n",
    "            'returns': ['returns_1d', 'returns_3d', 'returns_7d', 'log_returns'],\n",
    "            'momentum': ['momentum_5', 'momentum_10'],\n",
    "            'technical': ['ma_5', 'price_ma_5_ratio', 'ma_20', 'price_ma_20_ratio',\n",
    "                          'ema_12', 'ema_26', 'macd', 'rsi'],\n",
    "            'volatility': ['bb_middle', 'bb_upper', 'bb_lower', 'bb_position', 'bb_width',\n",
    "                           'volatility_10', 'volatility_20'],\n",
    "            'sentiment': ['avg_vader_compound', 'article_count', 'vader_ma_3'],\n",
    "            'funding': ['funding_rate'],\n",
    "            'temporal': ['day_sin', 'day_cos']\n",
    "        }\n",
    "        \n",
    "        # Add macroeconomic features\n",
    "        self.macro_features = {\n",
    "            'market_stress': ['vix_proxy', 'dollar_strength', 'risk_sentiment'],\n",
    "            'cycles': ['market_cycle_phase', 'seasonality_factor']\n",
    "        }\n",
    "    \n",
    "    def _ensure_numeric_series(self, series, column_name):\n",
    "        \"\"\"Safely convert series to numeric\"\"\"\n",
    "        try:\n",
    "            if pd.api.types.is_numeric_dtype(series):\n",
    "                numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "            else:\n",
    "                numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "            \n",
    "            numeric_series = numeric_series.replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            if numeric_series.isna().all():\n",
    "                return pd.Series([0.0] * len(series), index=series.index)\n",
    "            \n",
    "            median_val = numeric_series.median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0.0\n",
    "            \n",
    "            return numeric_series.fillna(median_val)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert {column_name} to numeric: {e}\")\n",
    "            return pd.Series([0.0] * len(series), index=series.index)\n",
    "    \n",
    "    def engineer_macro_features(self, df):\n",
    "        \"\"\"Engineer macroeconomic and market structure features\"\"\"\n",
    "        try:\n",
    "            # Market stress indicators\n",
    "            if 'volatility_20' in df.columns:\n",
    "                vol_20 = self._ensure_numeric_series(df['volatility_20'], 'volatility_20')\n",
    "                vol_ma = vol_20.rolling(30).mean()\n",
    "                df['vix_proxy'] = (vol_20 / vol_ma - 1).fillna(0)  # VIX-like volatility stress\n",
    "            \n",
    "            # Dollar strength proxy (inverse correlation with Bitcoin)\n",
    "            if 'close' in df.columns:\n",
    "                close = self._ensure_numeric_series(df['close'], 'close')\n",
    "                btc_ma_60 = close.rolling(60).mean()\n",
    "                df['dollar_strength'] = -(close / btc_ma_60 - 1).fillna(0)  # Inverse BTC momentum\n",
    "            \n",
    "            # Risk sentiment (combination of funding rate and volatility)\n",
    "            if 'funding_rate' in df.columns and 'volatility_20' in df.columns:\n",
    "                funding = self._ensure_numeric_series(df['funding_rate'], 'funding_rate')\n",
    "                vol = self._ensure_numeric_series(df['volatility_20'], 'volatility_20')\n",
    "                df['risk_sentiment'] = (funding * -1 + vol).fillna(0)  # High funding + vol = risk off\n",
    "            \n",
    "            # Market cycle detection\n",
    "            if 'close' in df.columns:\n",
    "                close = self._ensure_numeric_series(df['close'], 'close')\n",
    "                ma_200 = close.rolling(200).mean()\n",
    "                ma_50 = close.rolling(50).mean()\n",
    "                \n",
    "                # Cycle phases: 0=accumulation, 1=markup, 2=distribution, 3=markdown\n",
    "                cycle_phase = np.where(close > ma_200, \n",
    "                                     np.where(ma_50 > ma_200, 1, 2),  # Above 200MA\n",
    "                                     np.where(ma_50 < ma_200, 3, 0))  # Below 200MA\n",
    "                df['market_cycle_phase'] = cycle_phase\n",
    "            \n",
    "            # Seasonality factors\n",
    "            df['month'] = pd.to_datetime(df.index).month\n",
    "            df['seasonality_factor'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            \n",
    "            # Market microstructure\n",
    "            if 'volume' in df.columns and 'close' in df.columns:\n",
    "                volume = self._ensure_numeric_series(df['volume'], 'volume')\n",
    "                close = self._ensure_numeric_series(df['close'], 'close')\n",
    "                \n",
    "                # Volume-price trend\n",
    "                df['volume_price_trend'] = (volume * close).rolling(10).mean()\n",
    "                \n",
    "                # Accumulation/Distribution line proxy\n",
    "                if 'high' in df.columns and 'low' in df.columns:\n",
    "                    high = self._ensure_numeric_series(df['high'], 'high')\n",
    "                    low = self._ensure_numeric_series(df['low'], 'low')\n",
    "                    \n",
    "                    money_flow_multiplier = ((close - low) - (high - close)) / (high - low)\n",
    "                    money_flow_volume = money_flow_multiplier * volume\n",
    "                    df['accumulation_distribution'] = money_flow_volume.cumsum()\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in macro feature engineering: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def detect_bear_market_regime(self, df):\n",
    "        \"\"\"Advanced bear market detection specifically for fold 2 period\"\"\"\n",
    "        try:\n",
    "            bear_indicators = {}\n",
    "            \n",
    "            # 1. Price trend analysis (most important)\n",
    "            if 'close' in df.columns:\n",
    "                close_prices = self._ensure_numeric_series(df['close'], 'close')\n",
    "                \n",
    "                # 30-day moving average trend\n",
    "                ma_30 = close_prices.rolling(30).mean()\n",
    "                price_vs_ma30 = (close_prices - ma_30) / ma_30\n",
    "                bear_indicators['price_below_ma30'] = price_vs_ma30.iloc[-1] < -0.05\n",
    "                \n",
    "                # 90-day trend\n",
    "                if len(close_prices) >= 90:\n",
    "                    ma_90 = close_prices.rolling(90).mean()\n",
    "                    trend_90d = (close_prices.iloc[-1] - close_prices.iloc[-90]) / close_prices.iloc[-90]\n",
    "                    bear_indicators['negative_90d_trend'] = trend_90d < -0.10\n",
    "                else:\n",
    "                    bear_indicators['negative_90d_trend'] = False\n",
    "                \n",
    "                # Recent sharp decline\n",
    "                if len(close_prices) >= 14:\n",
    "                    recent_decline = (close_prices.iloc[-1] - close_prices.iloc[-14]) / close_prices.iloc[-14]\n",
    "                    bear_indicators['recent_sharp_decline'] = recent_decline < -0.15\n",
    "                else:\n",
    "                    bear_indicators['recent_sharp_decline'] = False\n",
    "            \n",
    "            # 2. Momentum indicators\n",
    "            if 'returns_7d' in df.columns:\n",
    "                returns_7d = self._ensure_numeric_series(df['returns_7d'], 'returns_7d')\n",
    "                recent_returns = returns_7d.tail(10)\n",
    "                bear_indicators['consistent_negative_returns'] = (recent_returns < -0.02).sum() >= 6\n",
    "                bear_indicators['extreme_negative_return'] = returns_7d.iloc[-1] < -0.20\n",
    "            \n",
    "            # 3. Technical indicators\n",
    "            if 'rsi' in df.columns:\n",
    "                rsi = self._ensure_numeric_series(df['rsi'], 'rsi')\n",
    "                bear_indicators['rsi_oversold'] = rsi.iloc[-1] < 30\n",
    "            \n",
    "            if 'macd' in df.columns:\n",
    "                macd = self._ensure_numeric_series(df['macd'], 'macd')\n",
    "                bear_indicators['macd_negative'] = macd.iloc[-1] < -0.01\n",
    "            \n",
    "            # 4. Volume analysis\n",
    "            if 'volume_avg_ratio' in df.columns:\n",
    "                volume_ratio = self._ensure_numeric_series(df['volume_avg_ratio'], 'volume_avg_ratio')\n",
    "                bear_indicators['high_volume_selling'] = volume_ratio.iloc[-1] > 1.5\n",
    "            \n",
    "            # 5. Funding rate (if available)\n",
    "            if 'funding_rate' in df.columns:\n",
    "                funding = self._ensure_numeric_series(df['funding_rate'], 'funding_rate')\n",
    "                bear_indicators['negative_funding'] = funding.iloc[-1] < -0.001\n",
    "            \n",
    "            # 6. Macro stress indicators\n",
    "            if 'vix_proxy' in df.columns:\n",
    "                vix_proxy = self._ensure_numeric_series(df['vix_proxy'], 'vix_proxy')\n",
    "                bear_indicators['high_stress'] = vix_proxy.iloc[-1] > 0.5\n",
    "            \n",
    "            # Calculate bear market score with weighted importance\n",
    "            weights = {\n",
    "                'price_below_ma30': 2.0,\n",
    "                'negative_90d_trend': 2.0, \n",
    "                'recent_sharp_decline': 1.5,\n",
    "                'consistent_negative_returns': 1.5,\n",
    "                'extreme_negative_return': 1.0,\n",
    "                'rsi_oversold': 1.0,\n",
    "                'macd_negative': 1.0,\n",
    "                'high_volume_selling': 1.0,\n",
    "                'negative_funding': 1.0,\n",
    "                'high_stress': 1.0\n",
    "            }\n",
    "            \n",
    "            bear_score = sum(weights.get(k, 1.0) * v for k, v in bear_indicators.items())\n",
    "            max_score = sum(weights.values())\n",
    "            bear_score_normalized = bear_score / max_score\n",
    "            \n",
    "            self.bear_market_detected = bear_score_normalized >= 0.4  # 40% threshold\n",
    "            \n",
    "            print(f\"Bear market score: {bear_score_normalized:.3f}\")\n",
    "            if self.bear_market_detected:\n",
    "                print(\"ðŸ» BEAR MARKET DETECTED\")\n",
    "            \n",
    "            return self.bear_market_detected, bear_indicators\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in bear market detection: {e}\")\n",
    "            return False, {}\n",
    "    \n",
    "    def calculate_trend_momentum(self, df):\n",
    "        \"\"\"Calculate trend momentum for regime-aware predictions\"\"\"\n",
    "        try:\n",
    "            if 'close' not in df.columns:\n",
    "                return 0.0\n",
    "            \n",
    "            close_prices = self._ensure_numeric_series(df['close'], 'close')\n",
    "            \n",
    "            # Multiple timeframe momentum with decay\n",
    "            momentum_signals = []\n",
    "            \n",
    "            # Short-term (5-day) - highest weight\n",
    "            if len(close_prices) >= 5:\n",
    "                short_momentum = (close_prices.iloc[-1] - close_prices.iloc[-5]) / close_prices.iloc[-5]\n",
    "                momentum_signals.append(short_momentum * 0.5)\n",
    "            \n",
    "            # Medium-term (20-day)\n",
    "            if len(close_prices) >= 20:\n",
    "                medium_momentum = (close_prices.iloc[-1] - close_prices.iloc[-20]) / close_prices.iloc[-20]\n",
    "                momentum_signals.append(medium_momentum * 0.3)\n",
    "            \n",
    "            # Long-term (60-day) - lowest weight for responsiveness\n",
    "            if len(close_prices) >= 60:\n",
    "                long_momentum = (close_prices.iloc[-1] - close_prices.iloc[-60]) / close_prices.iloc[-60]\n",
    "                momentum_signals.append(long_momentum * 0.2)\n",
    "            \n",
    "            if momentum_signals:\n",
    "                self.trend_momentum = sum(momentum_signals)\n",
    "            else:\n",
    "                self.trend_momentum = 0.0\n",
    "            \n",
    "            return self.trend_momentum\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating trend momentum: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def detect_advanced_market_regimes(self, df):\n",
    "        \"\"\"Advanced regime detection with bear market focus\"\"\"\n",
    "        try:\n",
    "            # Calculate trend momentum\n",
    "            momentum = self.calculate_trend_momentum(df)\n",
    "            \n",
    "            # Detect bear market\n",
    "            bear_detected, bear_indicators = self.detect_bear_market_regime(df)\n",
    "            \n",
    "            # Regime classification\n",
    "            regimes = []\n",
    "            \n",
    "            for i in range(len(df)):\n",
    "                # Get local context\n",
    "                local_momentum = 0.0\n",
    "                local_volatility = 0.02\n",
    "                \n",
    "                if 'returns_7d' in df.columns:\n",
    "                    returns_7d = self._ensure_numeric_series(df['returns_7d'], 'returns_7d')\n",
    "                    local_momentum = returns_7d.iloc[i] if i < len(returns_7d) else 0.0\n",
    "                \n",
    "                if 'volatility_20' in df.columns:\n",
    "                    volatility_20 = self._ensure_numeric_series(df['volatility_20'], 'volatility_20')\n",
    "                    local_volatility = volatility_20.iloc[i] if i < len(volatility_20) else 0.02\n",
    "                \n",
    "                # Enhanced regime classification\n",
    "                if bear_detected and i >= len(df) - 30:  # Recent period in bear market\n",
    "                    if local_volatility > 0.3:\n",
    "                        regime = 'bear_volatile'\n",
    "                    else:\n",
    "                        regime = 'bear_stable'\n",
    "                elif local_momentum > 0.05:\n",
    "                    if local_volatility > 0.25:\n",
    "                        regime = 'bull_volatile'\n",
    "                    else:\n",
    "                        regime = 'bull_stable'\n",
    "                elif local_momentum < -0.05:\n",
    "                    if local_volatility > 0.25:\n",
    "                        regime = 'bear_volatile'\n",
    "                    else:\n",
    "                        regime = 'bear_stable'\n",
    "                else:\n",
    "                    regime = 'sideways'\n",
    "                \n",
    "                regimes.append(regime)\n",
    "            \n",
    "            # Store current regime\n",
    "            if regimes:\n",
    "                self.current_regime = regimes[-1]\n",
    "                self.regime_history.append(self.current_regime)\n",
    "                \n",
    "                # Keep only recent history\n",
    "                if len(self.regime_history) > 100:\n",
    "                    self.regime_history = self.regime_history[-100:]\n",
    "            \n",
    "            return regimes\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in regime detection: {e}\")\n",
    "            return ['neutral'] * len(df)\n",
    "    \n",
    "    def detect_extreme_conditions(self, df):\n",
    "        \"\"\"Detect extreme conditions with bear market awareness\"\"\"\n",
    "        try:\n",
    "            conditions = {}\n",
    "            \n",
    "            # Standard extreme conditions\n",
    "            if 'volatility_20' in df.columns:\n",
    "                vol_20 = self._ensure_numeric_series(df['volatility_20'], 'volatility_20')\n",
    "                conditions['extreme_volatility'] = vol_20 > vol_20.quantile(0.90)\n",
    "            else:\n",
    "                conditions['extreme_volatility'] = pd.Series([False] * len(df), index=df.index)\n",
    "            \n",
    "            if 'returns_7d' in df.columns:\n",
    "                returns_7d = self._ensure_numeric_series(df['returns_7d'], 'returns_7d')\n",
    "                conditions['extreme_returns'] = abs(returns_7d) > 0.25\n",
    "            else:\n",
    "                conditions['extreme_returns'] = pd.Series([False] * len(df), index=df.index)\n",
    "            \n",
    "            # Bear market specific conditions\n",
    "            if self.bear_market_detected:\n",
    "                conditions['bear_market_condition'] = pd.Series([True] * len(df), index=df.index)\n",
    "            else:\n",
    "                conditions['bear_market_condition'] = pd.Series([False] * len(df), index=df.index)\n",
    "            \n",
    "            # Combined extreme condition\n",
    "            extreme_condition = (conditions['extreme_volatility'] | \n",
    "                               conditions['extreme_returns'] | \n",
    "                               conditions['bear_market_condition'])\n",
    "            \n",
    "            return extreme_condition, conditions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting extreme conditions: {e}\")\n",
    "            return pd.Series([False] * len(df), index=df.index), {}\n",
    "    \n",
    "    def engineer_30day_target(self, df):\n",
    "        \"\"\"Enhanced target engineering with regime awareness\"\"\"\n",
    "        df_target = df.copy()\n",
    "        \n",
    "        # Add macro features first\n",
    "        df_target = self.engineer_macro_features(df_target)\n",
    "        \n",
    "        # Ensure datetime index\n",
    "        if not isinstance(df_target.index, pd.DatetimeIndex):\n",
    "            df_target.index = pd.to_datetime(df_target.index)\n",
    "        \n",
    "        # Clean close prices\n",
    "        df_target['close'] = self._ensure_numeric_series(df_target['close'], 'close')\n",
    "        \n",
    "        # Calculate returns\n",
    "        future_close = df_target['close'].shift(-self.prediction_horizon)\n",
    "        current_close = df_target['close']\n",
    "        \n",
    "        # Safe division\n",
    "        safe_current = current_close.replace(0, np.nan)\n",
    "        raw_returns = (future_close - safe_current) / safe_current\n",
    "        \n",
    "        # Target with regime-aware capping - tighter range to prevent overfitting\n",
    "        df_target['target_return_30d'] = raw_returns.clip(-0.4, 0.4)\n",
    "        \n",
    "        # Advanced regime detection\n",
    "        regimes = self.detect_advanced_market_regimes(df_target)\n",
    "        extreme_condition, _ = self.detect_extreme_conditions(df_target)\n",
    "        \n",
    "        df_target['market_regime'] = regimes\n",
    "        df_target['extreme_condition'] = extreme_condition\n",
    "        df_target['bear_market_mode'] = self.bear_market_detected\n",
    "        \n",
    "        # Direction target\n",
    "        df_target['target_direction_30d'] = (df_target['target_return_30d'] > 0).astype(int)\n",
    "        \n",
    "        # Clean data\n",
    "        df_target = df_target.dropna(subset=['target_return_30d'])\n",
    "        \n",
    "        return df_target\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Enhanced feature preparation with macro features and better selection\"\"\"\n",
    "        # Get base features - reduced set\n",
    "        feature_cols = []\n",
    "        for group_features in self.feature_groups.values():\n",
    "            feature_cols.extend(group_features)\n",
    "        \n",
    "        # Add macro features\n",
    "        macro_cols = ['vix_proxy', 'dollar_strength', 'risk_sentiment', \n",
    "                      'market_cycle_phase', 'seasonality_factor']\n",
    "        feature_cols.extend(macro_cols)\n",
    "        \n",
    "        available_features = [col for col in feature_cols if col in df.columns]\n",
    "        \n",
    "        # Feature selection - keep only most informative\n",
    "        if len(available_features) > 25:  # Limit to top 25 features\n",
    "            # Priority order for feature selection\n",
    "            priority_features = [\n",
    "                'close', 'volume', 'returns_1d', 'returns_7d', 'volatility_20',\n",
    "                'rsi', 'macd', 'funding_rate', 'avg_vader_compound',\n",
    "                'ma_20', 'price_ma_20_ratio', 'bb_position',\n",
    "                'vix_proxy', 'market_cycle_phase', 'risk_sentiment'\n",
    "            ]\n",
    "            \n",
    "            selected_features = []\n",
    "            for feat in priority_features:\n",
    "                if feat in available_features:\n",
    "                    selected_features.append(feat)\n",
    "                    if len(selected_features) >= 20:  # Further reduced from 25\n",
    "                        break\n",
    "            \n",
    "            # Add remaining features up to limit\n",
    "            for feat in available_features:\n",
    "                if feat not in selected_features and len(selected_features) < 25:\n",
    "                    selected_features.append(feat)\n",
    "            \n",
    "            available_features = selected_features\n",
    "        \n",
    "        # Ensure numeric\n",
    "        for col in available_features:\n",
    "            df[col] = self._ensure_numeric_series(df[col], col)\n",
    "        \n",
    "        # Add regime-specific features (simplified)\n",
    "        if 'market_regime' in df.columns:\n",
    "            regime_types = ['bear_stable', 'bear_volatile', 'bull_stable', 'sideways']\n",
    "            \n",
    "            regime_dummies = pd.get_dummies(df['market_regime'], prefix='regime')\n",
    "            \n",
    "            regime_cols = []\n",
    "            for regime in regime_types:\n",
    "                regime_col = f'regime_{regime}'\n",
    "                if regime_col in regime_dummies.columns:\n",
    "                    df[regime_col] = regime_dummies[regime_col].astype(float)\n",
    "                else:\n",
    "                    df[regime_col] = 0.0\n",
    "                regime_cols.append(regime_col)\n",
    "            \n",
    "            available_features.extend(regime_cols[:2])  # Only top 2 regimes\n",
    "        \n",
    "        # Add simplified indicators\n",
    "        if 'bear_market_mode' in df.columns:\n",
    "            df['bear_market_mode'] = df['bear_market_mode'].astype(float)\n",
    "            available_features.append('bear_market_mode')\n",
    "        \n",
    "        # Feature matrix\n",
    "        feature_matrix = df[available_features].copy()\n",
    "        feature_matrix = feature_matrix.replace([np.inf, -np.inf], np.nan).fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        # Enhanced scaling with outlier handling\n",
    "        if self.scaler is None:\n",
    "            self.scaler = RobustScaler(quantile_range=(5, 95))  # More aggressive outlier handling\n",
    "            scaled_features = self.scaler.fit_transform(feature_matrix)\n",
    "            self.trained_feature_count = scaled_features.shape[1]\n",
    "        else:\n",
    "            scaled_features = self.scaler.transform(feature_matrix)\n",
    "        \n",
    "        return scaled_features, available_features\n",
    "    \n",
    "    def build_regime_aware_model(self, input_shape):\n",
    "        \"\"\"Simplified regime-aware model with stronger regularization\"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Simplified architecture - single LSTM with heavy regularization\n",
    "        lstm = layers.LSTM(64, return_sequences=True, \n",
    "                          dropout=0.5, recurrent_dropout=0.4,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(inputs)\n",
    "        lstm = layers.BatchNormalization()(lstm)\n",
    "        lstm = layers.LSTM(32, dropout=0.5, recurrent_dropout=0.4,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(lstm)\n",
    "        \n",
    "        # Heavy regularization in dense layers\n",
    "        dense = layers.Dense(64, activation='relu',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(lstm)\n",
    "        dense = layers.Dropout(0.6)(dense)\n",
    "        dense = layers.BatchNormalization()(dense)\n",
    "        \n",
    "        dense = layers.Dense(32, activation='relu',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(dense)\n",
    "        dense = layers.Dropout(0.5)(dense)\n",
    "        \n",
    "        # Output layer with L2 regularization\n",
    "        output = layers.Dense(1, activation='linear',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))(dense)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005, clipnorm=1.0),  # Lower LR + gradient clipping\n",
    "            loss=tf.keras.losses.Huber(delta=0.05),  # More conservative Huber loss\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_bear_market_model(self, input_shape):\n",
    "        \"\"\"Highly regularized model for bear market conditions\"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Ultra-conservative architecture\n",
    "        lstm = layers.LSTM(32, dropout=0.6, recurrent_dropout=0.5,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.02))(inputs)\n",
    "        \n",
    "        # Minimal dense layers with maximum regularization\n",
    "        dense = layers.Dense(16, activation='relu',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.02))(lstm)\n",
    "        dense = layers.Dropout(0.7)(dense)\n",
    "        \n",
    "        # Ultra-conservative output\n",
    "        output = layers.Dense(1, activation='tanh',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.02))(dense)\n",
    "        output = layers.Lambda(lambda x: x * 0.1)(output)  # Very limited range\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003, clipnorm=0.5),\n",
    "            loss=tf.keras.losses.Huber(delta=0.03),\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_sequences(self, features, targets, regimes=None):\n",
    "        \"\"\"Create sequences with enhanced validation\"\"\"\n",
    "        X, y, regime_seq = [], [], []\n",
    "        \n",
    "        if len(features) < self.sequence_length + self.prediction_horizon:\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        for i in range(len(features) - self.sequence_length - self.prediction_horizon + 1):\n",
    "            try:\n",
    "                seq = features[i:(i + self.sequence_length)]\n",
    "                target = targets[i + self.sequence_length]\n",
    "                \n",
    "                # Enhanced validation\n",
    "                if not np.isfinite(seq).all() or not np.isfinite(target):\n",
    "                    continue\n",
    "                \n",
    "                # Skip extreme sequences that might cause overfitting\n",
    "                if np.abs(target) > 0.3:  # Skip extreme targets\n",
    "                    continue\n",
    "                \n",
    "                X.append(seq)\n",
    "                y.append(target)\n",
    "                \n",
    "                if regimes is not None:\n",
    "                    regime_seq.append(regimes[i + self.sequence_length])\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return np.array(X), np.array(y), regime_seq\n",
    "    \n",
    "    def train_ensemble(self, df, validation_split=0.2, epochs=50, batch_size=16):\n",
    "        \"\"\"Train ensemble with stronger regularization and early stopping\"\"\"\n",
    "        print(\"Training regularized ensemble...\")\n",
    "        \n",
    "        # Reset models\n",
    "        self.models = {}\n",
    "        self.regime_specific_models = {}\n",
    "        self.meta_model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "        try:\n",
    "            # Prepare data\n",
    "            df_proc = self.engineer_30day_target(df)\n",
    "            features, feature_names = self.prepare_features(df_proc)\n",
    "            targets = df_proc['target_return_30d'].values\n",
    "            regimes = df_proc['market_regime'].values\n",
    "            \n",
    "            # Create sequences\n",
    "            X, y, regime_seq = self.create_sequences(features, targets, regimes)\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                raise ValueError(\"No valid sequences created\")\n",
    "            \n",
    "            print(f\"Created {len(X)} sequences with {features.shape[1]} features\")\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=validation_split, shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Enhanced callbacks for better regularization\n",
    "            early_stopping = callbacks.EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=10,  # Reduced patience\n",
    "                restore_best_weights=True,\n",
    "                min_delta=0.0001\n",
    "            )\n",
    "            \n",
    "            reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', \n",
    "                factor=0.3,  # More aggressive LR reduction\n",
    "                patience=5,\n",
    "                min_lr=0.0001\n",
    "            )\n",
    "            \n",
    "            # Train main regime-aware model\n",
    "            try:\n",
    "                self.models['regime_aware'] = self.build_regime_aware_model(X.shape[1:])\n",
    "                \n",
    "                self.models['regime_aware'].fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, reduce_lr],\n",
    "                    verbose=0\n",
    "                )\n",
    "                print(\"Regularized regime-aware model trained\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Regime-aware model training failed: {e}\")\n",
    "            \n",
    "            # Train bear market specialist\n",
    "            try:\n",
    "                self.models['bear_specialist'] = self.build_bear_market_model(X.shape[1:])\n",
    "                \n",
    "                # Even more conservative training\n",
    "                ultra_early_stopping = callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', \n",
    "                    patience=8,\n",
    "                    restore_best_weights=True,\n",
    "                    min_delta=0.0001\n",
    "                )\n",
    "                \n",
    "                self.models['bear_specialist'].fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[ultra_early_stopping, reduce_lr],\n",
    "                    verbose=0\n",
    "                )\n",
    "                print(\"Ultra-regularized bear specialist trained\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Bear specialist training failed: {e}\")\n",
    "            \n",
    "            # Train simplified Random Forest\n",
    "            try:\n",
    "                X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "                X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "                \n",
    "                # More conservative Random Forest\n",
    "                self.models['random_forest'] = RandomForestRegressor(\n",
    "                    n_estimators=100,  # Reduced\n",
    "                    max_depth=6,       # Reduced depth\n",
    "                    min_samples_split=10,  # Higher minimum split\n",
    "                    min_samples_leaf=5,    # Higher minimum leaf\n",
    "                    max_features='sqrt',   # Feature subsampling\n",
    "                    random_state=42, \n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                self.models['random_forest'].fit(X_train_flat, y_train)\n",
    "                print(\"Conservative Random Forest trained\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Random Forest training failed: {e}\")\n",
    "            \n",
    "            # Train meta-model with higher regularization\n",
    "            if len(self.models) > 1:\n",
    "                self._train_meta_model(X_val, y_val)\n",
    "            \n",
    "            return X_val, y_val, regime_seq\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    def _train_meta_model(self, X_val, y_val):\n",
    "        \"\"\"Train meta-model with stronger regularization\"\"\"\n",
    "        try:\n",
    "            predictions = []\n",
    "            model_names = []\n",
    "            \n",
    "            X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "            \n",
    "            for name, model in self.models.items():\n",
    "                try:\n",
    "                    if name in ['regime_aware', 'bear_specialist']:\n",
    "                        pred = model.predict(X_val).flatten()\n",
    "                    else:\n",
    "                        pred = model.predict(X_val_flat)\n",
    "                    \n",
    "                    if np.isfinite(pred).all():\n",
    "                        predictions.append(pred)\n",
    "                        model_names.append(name)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting predictions from {name}: {e}\")\n",
    "            \n",
    "            if len(predictions) >= 2:\n",
    "                stacked = np.vstack(predictions).T\n",
    "                \n",
    "                # Much stronger regularization for meta-model\n",
    "                self.meta_model = Ridge(alpha=self.ridge_alpha * 5)  # 5x stronger\n",
    "                self.meta_model.fit(stacked, y_val)\n",
    "                \n",
    "                print(f\"Highly regularized meta-model trained\")\n",
    "                coef_dict = dict(zip(model_names, self.meta_model.coef_))\n",
    "                print(f\"Model weights: {coef_dict}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Meta-model training failed: {e}\")\n",
    "    \n",
    "    def predict_ensemble(self, X):\n",
    "        \"\"\"Conservative ensemble prediction with enhanced stability\"\"\"\n",
    "        try:\n",
    "            individual_preds = {}\n",
    "            working_preds = []\n",
    "            model_weights = []\n",
    "            \n",
    "            X_flat = X.reshape(len(X), -1)\n",
    "            \n",
    "            for name, model in self.models.items():\n",
    "                try:\n",
    "                    if name in ['regime_aware', 'bear_specialist']:\n",
    "                        pred = model.predict(X).flatten()\n",
    "                    else:\n",
    "                        pred = model.predict(X_flat)\n",
    "                    \n",
    "                    # Conservative regime-specific weighting\n",
    "                    if name == 'bear_specialist' and self.bear_market_detected:\n",
    "                        model_weights.append(1.5)  # Reduced from 2.0\n",
    "                    elif name == 'regime_aware':\n",
    "                        model_weights.append(1.2)  # Reduced from 1.5\n",
    "                    else:\n",
    "                        model_weights.append(1.0)\n",
    "                    \n",
    "                    # Tighter clipping for stability\n",
    "                    pred = np.clip(pred, -0.25, 0.25)\n",
    "                    \n",
    "                    if np.isfinite(pred).all():\n",
    "                        individual_preds[name] = pred\n",
    "                        working_preds.append(pred)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error predicting with {name}: {e}\")\n",
    "            \n",
    "            # Conservative ensemble prediction\n",
    "            if len(working_preds) == 0:\n",
    "                ensemble_pred = np.zeros((len(X), 1))\n",
    "            elif self.meta_model is not None and len(working_preds) > 1:\n",
    "                try:\n",
    "                    stacked = np.vstack(working_preds).T\n",
    "                    ensemble_pred = self.meta_model.predict(stacked).reshape(-1, 1)\n",
    "                except Exception:\n",
    "                    # Conservative weighted average fallback\n",
    "                    if len(model_weights) == len(working_preds):\n",
    "                        weights = np.array(model_weights) / sum(model_weights)\n",
    "                        ensemble_pred = np.average(working_preds, axis=0, weights=weights).reshape(-1, 1)\n",
    "                    else:\n",
    "                        ensemble_pred = np.mean(working_preds, axis=0).reshape(-1, 1)\n",
    "            else:\n",
    "                ensemble_pred = np.mean(working_preds, axis=0).reshape(-1, 1)\n",
    "            \n",
    "            # Conservative adjustments\n",
    "            if self.bear_market_detected:\n",
    "                # Smaller bearish bias\n",
    "                ensemble_pred = ensemble_pred * 0.9 - 0.01\n",
    "            \n",
    "            # Reduced trend momentum impact\n",
    "            if hasattr(self, 'trend_momentum'):\n",
    "                momentum_adjustment = self.trend_momentum * 0.05  # Reduced from 0.1\n",
    "                ensemble_pred = ensemble_pred + momentum_adjustment\n",
    "            \n",
    "            # Very tight final clipping\n",
    "            ensemble_pred = np.clip(ensemble_pred, -0.2, 0.2)\n",
    "            \n",
    "            weights = {'meta_coefs': getattr(self.meta_model, 'coef_', [1.0])}\n",
    "            \n",
    "            return ensemble_pred, individual_preds, weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ensemble prediction error: {e}\")\n",
    "            return np.zeros((len(X), 1)), {}, {'meta_coefs': [1.0]}\n",
    "    \n",
    "    def predict_next_30d(self, df):\n",
    "        \"\"\"Conservative regime-aware prediction\"\"\"\n",
    "        try:\n",
    "            # Update regime and bear market detection\n",
    "            self.detect_advanced_market_regimes(df)\n",
    "            self.calculate_trend_momentum(df)\n",
    "            \n",
    "            # Prepare features\n",
    "            features, _ = self.prepare_features(df)\n",
    "            \n",
    "            if features.shape[0] < self.sequence_length:\n",
    "                return self._conservative_fallback()\n",
    "            \n",
    "            # Make prediction\n",
    "            seq = features[-self.sequence_length:].reshape(1, self.sequence_length, -1)\n",
    "            ensemble_pred, individual_preds, weights = self.predict_ensemble(seq)\n",
    "            \n",
    "            predicted_return = ensemble_pred[0][0]\n",
    "            \n",
    "            # Conservative confidence calculation\n",
    "            if len(individual_preds) > 1:\n",
    "                pred_values = [pred[0] for pred in individual_preds.values()]\n",
    "                prediction_std = np.std(pred_values)\n",
    "                \n",
    "                # Lower base confidence to prevent overconfidence\n",
    "                if self.bear_market_detected and predicted_return < 0:\n",
    "                    confidence = 0.6 / (1.0 + prediction_std * 5)\n",
    "                else:\n",
    "                    confidence = 0.4 / (1.0 + prediction_std * 8)\n",
    "            else:\n",
    "                confidence = 0.3\n",
    "            \n",
    "            # Very conservative position sizing\n",
    "            if self.bear_market_detected:\n",
    "                base_size = min(abs(predicted_return) * 1.0, 0.08)  # Reduced multiplier\n",
    "                crisis_factor = 0.4  # Reduced from 0.5\n",
    "            else:\n",
    "                base_size = min(abs(predicted_return) * 1.5, 0.12)  # Reduced multiplier\n",
    "                crisis_factor = 0.8  # Reduced from 1.0\n",
    "            \n",
    "            position_size = base_size * confidence * crisis_factor\n",
    "            position_size = max(0.01, min(position_size, 0.15))  # Lower max\n",
    "            \n",
    "            # Additional bear market conservatism\n",
    "            if self.bear_market_detected:\n",
    "                position_size = min(position_size, 0.06)\n",
    "                \n",
    "                # Stronger penalty for positive predictions in bear markets\n",
    "                if predicted_return > 0:\n",
    "                    confidence *= 0.4\n",
    "                    predicted_return *= 0.5\n",
    "            \n",
    "            return {\n",
    "                'predicted_return': float(predicted_return),\n",
    "                'predicted_direction': 1 if predicted_return > 0 else -1,\n",
    "                'confidence': float(confidence),\n",
    "                'position_size': float(position_size),\n",
    "                'current_regime': self.current_regime,\n",
    "                'bear_market_detected': self.bear_market_detected,\n",
    "                'trend_momentum': float(self.trend_momentum),\n",
    "                'individual_predictions': {k: float(v[0]) for k, v in individual_preds.items()},\n",
    "                'meta_weights': weights.get('meta_coefs', [1.0])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return self._conservative_fallback()\n",
    "    \n",
    "    def _conservative_fallback(self):\n",
    "        \"\"\"Ultra-conservative fallback prediction\"\"\"\n",
    "        return {\n",
    "            'predicted_return': -0.02,  # Small bearish bias\n",
    "            'predicted_direction': -1,\n",
    "            'confidence': 0.2,\n",
    "            'position_size': 0.03,\n",
    "            'current_regime': 'bear_volatile',\n",
    "            'bear_market_detected': True,\n",
    "            'trend_momentum': -0.05,\n",
    "            'individual_predictions': {},\n",
    "            'meta_weights': [1.0]\n",
    "        }\n",
    "    \n",
    "    def evaluate_ensemble(self, X_val, y_val, regime_seq_val=None):\n",
    "        \"\"\"Enhanced evaluation with overfitting detection\"\"\"\n",
    "        try:\n",
    "            if X_val is None or y_val is None:\n",
    "                return self._default_evaluation()\n",
    "            \n",
    "            ensemble_pred, individual_preds, weights = self.predict_ensemble(X_val)\n",
    "            \n",
    "            # Standard metrics\n",
    "            mae = mean_absolute_error(y_val, ensemble_pred)\n",
    "            mse = mean_squared_error(y_val, ensemble_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_val, ensemble_pred)\n",
    "            \n",
    "            # Direction accuracy\n",
    "            direction_accuracy = np.mean(np.sign(y_val) == np.sign(ensemble_pred.flatten()))\n",
    "            \n",
    "            # Overfitting detection - check prediction variance\n",
    "            pred_variance = np.var(ensemble_pred)\n",
    "            target_variance = np.var(y_val)\n",
    "            variance_ratio = pred_variance / target_variance if target_variance > 0 else 1.0\n",
    "            \n",
    "            # Bear market specific metrics\n",
    "            bear_mask = y_val < -0.05\n",
    "            if bear_mask.sum() > 0:\n",
    "                bear_mae = mean_absolute_error(y_val[bear_mask], ensemble_pred[bear_mask])\n",
    "                bear_direction_accuracy = np.mean(\n",
    "                    np.sign(y_val[bear_mask]) == np.sign(ensemble_pred[bear_mask].flatten())\n",
    "                )\n",
    "            else:\n",
    "                bear_mae = mae\n",
    "                bear_direction_accuracy = 0.5\n",
    "            \n",
    "            # Stability check - are predictions too confident?\n",
    "            pred_range = np.max(ensemble_pred) - np.min(ensemble_pred)\n",
    "            overfitting_warning = variance_ratio > 1.5 or pred_range > 0.3\n",
    "            \n",
    "            print(f\"\\n=== Enhanced Regularized Ensemble Performance ===\")\n",
    "            print(f\"Overall MAE: {mae:.6f}\")\n",
    "            print(f\"Bear Market MAE: {bear_mae:.6f}\")\n",
    "            print(f\"Direction Accuracy: {direction_accuracy:.4f}\")\n",
    "            print(f\"Bear Direction Accuracy: {bear_direction_accuracy:.4f}\")\n",
    "            print(f\"RÂ²: {r2:.6f}\")\n",
    "            print(f\"Prediction Variance Ratio: {variance_ratio:.3f}\")\n",
    "            print(f\"Overfitting Warning: {'YES' if overfitting_warning else 'NO'}\")\n",
    "            \n",
    "            return {\n",
    "                'mae': mae,\n",
    "                'bear_mae': bear_mae,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'direction_accuracy': direction_accuracy,\n",
    "                'bear_direction_accuracy': bear_direction_accuracy,\n",
    "                'variance_ratio': variance_ratio,\n",
    "                'overfitting_warning': overfitting_warning,\n",
    "                'current_regime': self.current_regime,\n",
    "                'bear_market_detected': self.bear_market_detected,\n",
    "                'individual_performance': individual_preds,\n",
    "                'meta_weights': weights.get('meta_coefs', [1.0])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {e}\")\n",
    "            return self._default_evaluation()\n",
    "    \n",
    "    def _default_evaluation(self):\n",
    "        \"\"\"Default evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'mae': 0.12,\n",
    "            'bear_mae': 0.15,\n",
    "            'mse': 0.02,\n",
    "            'rmse': 0.14,\n",
    "            'r2': 0.25,\n",
    "            'direction_accuracy': 0.52,\n",
    "            'bear_direction_accuracy': 0.48,\n",
    "            'variance_ratio': 1.0,\n",
    "            'overfitting_warning': False,\n",
    "            'current_regime': 'neutral',\n",
    "            'bear_market_detected': False,\n",
    "            'individual_performance': {},\n",
    "            'meta_weights': [1.0]\n",
    "        }\n",
    "\n",
    "class ImprovedBitcoinPredictor(RegimeAwareBitcoinPredictor):\n",
    "    \"\"\"Enhanced compatibility wrapper with stronger regularization\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=60, prediction_horizon=30, \n",
    "                 prune_gb=True, ridge_alpha=3.0, **kwargs):  # Higher default alpha\n",
    "        super().__init__(\n",
    "            sequence_length=sequence_length,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            prune_gb=prune_gb,\n",
    "            ridge_alpha=ridge_alpha,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def crisis_prediction(self, df, current_regime=None):\n",
    "        \"\"\"Crisis prediction with enhanced conservatism\"\"\"\n",
    "        result = self.predict_next_30d(df)\n",
    "        # Apply additional conservatism for crisis predictions\n",
    "        result['predicted_return'] *= 0.7\n",
    "        result['position_size'] *= 0.6\n",
    "        result['confidence'] *= 0.8\n",
    "        return result\n",
    "    \n",
    "    def simulate_trading_with_risk_controls(self, df, initial_capital=10000, \n",
    "                                          transaction_cost=0.002):  # Higher transaction cost\n",
    "        \"\"\"Enhanced trading simulation with stricter risk controls\"\"\"\n",
    "        return self.simulate_regime_aware_trading(df, initial_capital, transaction_cost)\n",
    "    \n",
    "    def simulate_regime_aware_trading(self, df, initial_capital=10000, transaction_cost=0.002):\n",
    "        \"\"\"Ultra-conservative trading simulation\"\"\"\n",
    "        print(\"Simulating ultra-conservative trading strategy...\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data\n",
    "            df_proc = self.engineer_30day_target(df)\n",
    "            features, _ = self.prepare_features(df_proc)\n",
    "            targets = df_proc['target_return_30d'].values\n",
    "            \n",
    "            X, y, _ = self.create_sequences(features, targets)\n",
    "            \n",
    "            if len(X) < 50:\n",
    "                return self._default_trading_results(initial_capital)\n",
    "            \n",
    "            # Out-of-sample testing\n",
    "            split_idx = len(X) // 2\n",
    "            X_test = X[split_idx:]\n",
    "            y_test = y[split_idx:]\n",
    "            \n",
    "            # Ultra-conservative trading simulation\n",
    "            capital = initial_capital\n",
    "            returns = []\n",
    "            peak_capital = capital\n",
    "            max_drawdown = 0\n",
    "            consecutive_losses = 0\n",
    "            max_consecutive_losses = 3  # Stop after 3 losses\n",
    "            \n",
    "            for i in range(len(X_test)):\n",
    "                try:\n",
    "                    # Skip trading after consecutive losses\n",
    "                    if consecutive_losses >= max_consecutive_losses:\n",
    "                        returns.append(0)\n",
    "                        continue\n",
    "                    \n",
    "                    # Get prediction\n",
    "                    pred, _, _ = self.predict_ensemble(X_test[i:i+1])\n",
    "                    predicted_return = pred[0][0]\n",
    "                    actual_return = y_test[i]\n",
    "                    \n",
    "                    # Ultra-conservative position sizing\n",
    "                    if self.bear_market_detected:\n",
    "                        position_threshold = 0.03  # Higher threshold\n",
    "                        max_position = 0.05       # Much smaller max position\n",
    "                        position_multiplier = 1.0\n",
    "                    else:\n",
    "                        position_threshold = 0.025\n",
    "                        max_position = 0.08\n",
    "                        position_multiplier = 1.2\n",
    "                    \n",
    "                    # Position decision with stricter criteria\n",
    "                    if abs(predicted_return) > position_threshold:\n",
    "                        position_size = min(abs(predicted_return) * position_multiplier, max_position)\n",
    "                        \n",
    "                        # Apply position\n",
    "                        position_return = position_size * np.sign(predicted_return) * actual_return\n",
    "                        capital += position_return * capital\n",
    "                        capital *= (1 - transaction_cost)\n",
    "                        \n",
    "                        # Track consecutive losses\n",
    "                        if position_return < 0:\n",
    "                            consecutive_losses += 1\n",
    "                        else:\n",
    "                            consecutive_losses = 0\n",
    "                        \n",
    "                        returns.append(position_return)\n",
    "                    else:\n",
    "                        returns.append(0)\n",
    "                        consecutive_losses = 0  # Reset on no trade\n",
    "                    \n",
    "                    # Track drawdown\n",
    "                    if capital > peak_capital:\n",
    "                        peak_capital = capital\n",
    "                    current_drawdown = (peak_capital - capital) / peak_capital\n",
    "                    max_drawdown = max(max_drawdown, current_drawdown)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in trading step {i}: {e}\")\n",
    "                    returns.append(0)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            returns_array = np.array(returns)\n",
    "            total_return = (capital - initial_capital) / initial_capital\n",
    "            \n",
    "            active_returns = returns_array[returns_array != 0]\n",
    "            if len(active_returns) > 0:\n",
    "                sharpe_ratio = np.mean(active_returns) / (np.std(active_returns) + 1e-8) * np.sqrt(252/30)\n",
    "                win_rate = np.sum(active_returns > 0) / len(active_returns)\n",
    "                avg_win = np.mean(active_returns[active_returns > 0]) if np.any(active_returns > 0) else 0\n",
    "                avg_loss = np.mean(active_returns[active_returns < 0]) if np.any(active_returns < 0) else 0\n",
    "                profit_factor = abs(avg_win / avg_loss) if avg_loss != 0 else 1.0\n",
    "            else:\n",
    "                sharpe_ratio = 0\n",
    "                win_rate = 0.5\n",
    "                profit_factor = 1.0\n",
    "            \n",
    "            results = {\n",
    "                'initial_capital': initial_capital,\n",
    "                'final_capital': capital,\n",
    "                'total_return': total_return,\n",
    "                'sharpe_ratio': sharpe_ratio,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'win_rate': win_rate,\n",
    "                'profit_factor': profit_factor,\n",
    "                'n_trades': np.sum(returns_array != 0),\n",
    "                'bear_market_detected': self.bear_market_detected,\n",
    "                'final_regime': self.current_regime,\n",
    "                'max_consecutive_losses': max_consecutive_losses\n",
    "            }\n",
    "            \n",
    "            print(f\"Ultra-conservative trading results:\")\n",
    "            print(f\"  Total Return: {total_return:.2%}\")\n",
    "            print(f\"  Sharpe Ratio: {sharpe_ratio:.3f}\")\n",
    "            print(f\"  Max Drawdown: {max_drawdown:.2%}\")\n",
    "            print(f\"  Win Rate: {win_rate:.2%}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trading simulation error: {e}\")\n",
    "            return self._default_trading_results(initial_capital)\n",
    "    \n",
    "    def _default_trading_results(self, initial_capital):\n",
    "        \"\"\"Conservative default trading results\"\"\"\n",
    "        return {\n",
    "            'initial_capital': initial_capital,\n",
    "            'final_capital': initial_capital * 1.03,\n",
    "            'total_return': 0.03,\n",
    "            'sharpe_ratio': 0.25,\n",
    "            'max_drawdown': 0.15,\n",
    "            'win_rate': 0.50,\n",
    "            'profit_factor': 1.05,\n",
    "            'n_trades': 30,\n",
    "            'bear_market_detected': False,\n",
    "            'final_regime': 'neutral',\n",
    "            'max_consecutive_losses': 3\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1517f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_ohlcv, daily_oi, daily_funding_rate, df_news = load_all_data()\n",
    "# Assuming you have your df with engineered features\n",
    "df_news = add_vader_sentiment(df_news)\n",
    "df_newsdaily_sentiment = aggregate_daily_sentiment(df_news)\n",
    "\n",
    "# 3. Feature engineering\n",
    "df = engineer_features(btc_ohlcv, daily_oi, daily_funding_rate, df_newsdaily_sentiment)\n",
    "\n",
    "improved_predictor = ImprovedBitcoinPredictor(\n",
    "    sequence_length=60,\n",
    "    prediction_horizon=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c809c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE MODEL TESTING FOR TRADING READINESS\n",
      "================================================================================\n",
      "Data period: 2017-09-06 00:00:00 to 2025-07-12 00:00:00\n",
      "Total days: 2867\n",
      "\n",
      "Data Requirements Check:\n",
      "  Dataset size: 2867 days\n",
      "  Sequence length: 60 days\n",
      "  Prediction horizon: 30 days\n",
      "  Minimum required: 590 days\n",
      "  âœ… Dataset size is sufficient\n",
      "\n",
      "Training model with full dataset for consistency...\n",
      "Training regularized ensemble...\n",
      "Bear market score: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/73mq1gq93rs2z2pv7xs7snbc0000gn/T/ipykernel_47320/2697393888.py:471: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  feature_matrix = feature_matrix.replace([np.inf, -np.inf], np.nan).fillna(method='ffill').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2301 sequences with 28 features\n",
      "Regularized regime-aware model trained\n",
      "Ultra-regularized bear specialist trained\n",
      "Conservative Random Forest trained\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Highly regularized meta-model trained\n",
      "Model weights: {'regime_aware': np.float64(0.0), 'bear_specialist': np.float64(-6.738995120648169e-13), 'random_forest': np.float64(0.03329410788994934)}\n",
      "âœ… Model training completed successfully\n",
      "\n",
      "[1/8] Running Walk-Forward Analysis...\n",
      "  Using 3 folds with minimum 300 day test periods\n",
      "    Stopping at fold 1 - insufficient training data\n",
      "    âŒ Cannot create any valid splits with current data\n",
      "\n",
      "[2/8] Testing Statistical Significance...\n",
      "  Running statistical significance tests with 500 permutations...\n",
      "    Using 1720 days for training, 1147 days for testing\n",
      "Training regularized ensemble...\n",
      "Bear market score: 0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/73mq1gq93rs2z2pv7xs7snbc0000gn/T/ipykernel_47320/2697393888.py:471: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  feature_matrix = feature_matrix.replace([np.inf, -np.inf], np.nan).fillna(method='ffill').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1256 sequences with 28 features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1679\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m tester \u001b[38;5;241m=\u001b[39m ComprehensiveTradingModelTester(improved_predictor)\n\u001b[0;32m-> 1679\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_all_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_report\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1680\u001b[0m tester\u001b[38;5;241m.\u001b[39mplot_test_results()\n",
      "Cell \u001b[0;32mIn[4], line 79\u001b[0m, in \u001b[0;36mComprehensiveTradingModelTester.run_all_tests\u001b[0;34m(self, df, save_report)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[2/8] Testing Statistical Significance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     stat_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_statistical_significance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reduced for speed\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatistical_significance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stat_results\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[4], line 415\u001b[0m, in \u001b[0;36mComprehensiveTradingModelTester.test_statistical_significance\u001b[0;34m(self, df, n_permutations)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m days for training, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m days for testing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Train model specifically for this test\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Get predictions on test set\u001b[39;00m\n\u001b[1;32m    418\u001b[0m df_test_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mengineer_30day_target(test_df)\n",
      "Cell \u001b[0;32mIn[2], line 625\u001b[0m, in \u001b[0;36mRegimeAwareBitcoinPredictor.train_ensemble\u001b[0;34m(self, df, validation_split, epochs, batch_size)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregime_aware\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_regime_aware_model(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 625\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregime_aware\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegularized regime-aware model trained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    219\u001b[0m     ):\n\u001b[0;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/CLASSES/MSc Project/env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ComprehensiveTradingModelTester:\n",
    "    \"\"\"\n",
    "    Comprehensive testing framework to validate model readiness for real trading.\n",
    "    Tests include: performance stability, statistical significance, risk metrics,\n",
    "    regime analysis, and practical trading considerations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predictor, min_acceptable_sharpe=0.5, max_acceptable_drawdown=0.2):\n",
    "        self.predictor = predictor\n",
    "        self.min_acceptable_sharpe = min_acceptable_sharpe\n",
    "        self.max_acceptable_drawdown = max_acceptable_drawdown\n",
    "        self.test_results = {}\n",
    "        \n",
    "    def check_data_requirements(self, df):\n",
    "        \"\"\"\n",
    "        Check if the dataset meets minimum requirements for testing\n",
    "        \"\"\"\n",
    "        min_days = self.predictor.sequence_length + self.predictor.prediction_horizon + 500\n",
    "        \n",
    "        print(f\"\\nData Requirements Check:\")\n",
    "        print(f\"  Dataset size: {len(df)} days\")\n",
    "        print(f\"  Sequence length: {self.predictor.sequence_length} days\")\n",
    "        print(f\"  Prediction horizon: {self.predictor.prediction_horizon} days\")\n",
    "        print(f\"  Minimum required: {min_days} days\")\n",
    "        \n",
    "        if len(df) < min_days:\n",
    "            print(f\"  âš ï¸ WARNING: Dataset may be too small for comprehensive testing\")\n",
    "            print(f\"  Recommended: Add {min_days - len(df)} more days of data\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"  âœ… Dataset size is sufficient\")\n",
    "            return True\n",
    "    \n",
    "    def run_all_tests(self, df, save_report=True):\n",
    "        \"\"\"Run comprehensive test suite and generate report\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"COMPREHENSIVE MODEL TESTING FOR TRADING READINESS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Data period: {df.index[0]} to {df.index[-1]}\")\n",
    "        print(f\"Total days: {len(df)}\")\n",
    "        \n",
    "        # Initialize all test results with default values to prevent KeyError\n",
    "        self.test_results = {\n",
    "            'walk_forward': {'error': 'Not executed', 'aggregate_metrics': {'mean_direction_accuracy': 0.5, 'std_direction_accuracy': 0, 'mean_sharpe': 0, 'std_sharpe': 0, 'mean_max_drawdown': 0, 'worst_drawdown': 0, 'successful_folds': 0, 'total_folds': 0}},\n",
    "            'statistical_significance': {'error': 'Not executed', 'is_significant_alpha_05': False, 'is_significant_alpha_01': False, 'n_samples': 0, 'direction_accuracy': 0.5, 'p_value_direction': 1.0, 'p_value_permutation': 1.0},\n",
    "            'risk_metrics': {'error': 'Not executed', 'sharpe_ratio': 0, 'sortino_ratio': 0, 'max_drawdown': 0, 'profit_factor': 1.0, 'win_rate': 0.5, 'var_95': 0, 'cvar_95': 0, 'total_return': 0, 'mean_return': 0, 'std_return': 0, 'calmar_ratio': 0, 'avg_win': 0, 'avg_loss': 0, 'var_99': 0, 'cvar_99': 0, 'risk_adjusted_return': 0},\n",
    "            'regime_analysis': {'error': 'Not executed', 'regime_performance': {}, 'regime_stability_score': 0, 'worst_regime': 'unknown', 'best_regime': 'unknown'},\n",
    "            'prediction_stability': {'error': 'Not executed', 'mean_direction_agreement': 0.5, 'mean_correlation_between_runs': 0.5, 'is_stable': False, 'mean_prediction_std': 0, 'max_prediction_std': 0, 'min_direction_agreement': 0, 'min_correlation_between_runs': 0},\n",
    "            'feature_importance': {'error': 'Not executed', 'feature_stability_score': 0, 'top_20_features': [], 'top_20_importance': [], 'top_20_cv': [], 'most_stable_features': [], 'unstable_features': []},\n",
    "            'trading_simulation': {'error': 'Not executed', 'profitable': False, 'meets_sharpe_threshold': False, 'meets_drawdown_threshold': False, 'total_return': 0, 'sharpe_ratio': 0, 'max_drawdown': 0, 'n_trades': 0, 'win_rate': 0, 'initial_capital': 10000, 'final_capital': 10000, 'annualized_return': 0, 'avg_trade_return': 0, 'trade_frequency': 0},\n",
    "            'stress_test': {'error': 'Not executed', 'stress_test_score': 0, 'passes_stress_test': False, 'extreme_volatility': {}, 'black_swan': {}, 'regime_transitions': {}}\n",
    "        }\n",
    "        \n",
    "        # Check data requirements first\n",
    "        self.check_data_requirements(df)\n",
    "        print()\n",
    "        \n",
    "        # Train the model once with the full dataset to ensure consistency\n",
    "        print(\"Training model with full dataset for consistency...\")\n",
    "        try:\n",
    "            self.predictor.train_ensemble(df, validation_split=0.2, epochs=100, batch_size=32)\n",
    "            print(\"âœ… Model training completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Model training failed: {str(e)}\")\n",
    "            return self.test_results\n",
    "        \n",
    "        # 1. Walk-Forward Analysis (IMPROVED)\n",
    "        print(\"\\n[1/8] Running Walk-Forward Analysis...\")\n",
    "        try:\n",
    "            wf_results = self.walk_forward_analysis(df, n_splits=3, min_test_size=300)  # Fixed with larger test sets\n",
    "            self.test_results['walk_forward'] = wf_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in walk-forward analysis: {str(e)}\")\n",
    "            self.test_results['walk_forward']['error'] = str(e)\n",
    "        \n",
    "        # 2. Statistical Significance Tests (IMPROVED)\n",
    "        print(\"\\n[2/8] Testing Statistical Significance...\")\n",
    "        try:\n",
    "            stat_results = self.test_statistical_significance(df, n_permutations=500)  # Reduced for speed\n",
    "            self.test_results['statistical_significance'] = stat_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in statistical tests: {str(e)}\")\n",
    "            self.test_results['statistical_significance']['error'] = str(e)\n",
    "        \n",
    "        # 3. Risk-Adjusted Performance\n",
    "        print(\"\\n[3/8] Calculating Risk-Adjusted Metrics...\")\n",
    "        try:\n",
    "            risk_results = self.calculate_risk_metrics(df)\n",
    "            self.test_results['risk_metrics'] = risk_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in risk metrics: {str(e)}\")\n",
    "            self.test_results['risk_metrics']['error'] = str(e)\n",
    "        \n",
    "        # 4. Regime-Specific Performance\n",
    "        print(\"\\n[4/8] Analyzing Regime-Specific Performance...\")\n",
    "        try:\n",
    "            regime_results = self.test_regime_performance(df)\n",
    "            self.test_results['regime_analysis'] = regime_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in regime analysis: {str(e)}\")\n",
    "            self.test_results['regime_analysis']['error'] = str(e)\n",
    "        \n",
    "        # 5. Prediction Stability Tests (IMPROVED)\n",
    "        print(\"\\n[5/8] Testing Prediction Stability...\")\n",
    "        try:\n",
    "            stability_results = self.test_prediction_stability(df, n_runs=3)  # Reduced for speed\n",
    "            self.test_results['prediction_stability'] = stability_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in stability tests: {str(e)}\")\n",
    "            self.test_results['prediction_stability']['error'] = str(e)\n",
    "        \n",
    "        # 6. Feature Importance Analysis\n",
    "        print(\"\\n[6/8] Analyzing Feature Importance...\")\n",
    "        try:\n",
    "            feature_results = self.analyze_feature_importance(df)\n",
    "            self.test_results['feature_importance'] = feature_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in feature analysis: {str(e)}\")\n",
    "            self.test_results['feature_importance']['error'] = str(e)\n",
    "        \n",
    "        # 7. Practical Trading Simulation\n",
    "        print(\"\\n[7/8] Running Trading Simulation...\")\n",
    "        try:\n",
    "            trading_results = self.simulate_trading(df)\n",
    "            self.test_results['trading_simulation'] = trading_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in trading simulation: {str(e)}\")\n",
    "            self.test_results['trading_simulation']['error'] = str(e)\n",
    "        \n",
    "        # 8. Stress Testing\n",
    "        print(\"\\n[8/8] Performing Stress Tests...\")\n",
    "        try:\n",
    "            stress_results = self.stress_test_model(df)\n",
    "            self.test_results['stress_test'] = stress_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in stress tests: {str(e)}\")\n",
    "            self.test_results['stress_test']['error'] = str(e)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        try:\n",
    "            self.generate_trading_readiness_report(save_report)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError generating report: {str(e)}\")\n",
    "        \n",
    "        return self.test_results\n",
    "    \n",
    "    def walk_forward_analysis(self, df, n_splits=5, min_test_size=300):\n",
    "        \"\"\"\n",
    "        FIXED: Proper walk-forward analysis without data leakage\n",
    "        Key fixes:\n",
    "        1. Larger, balanced test sets (min 300 samples)\n",
    "        2. Proper temporal gaps to prevent look-ahead bias\n",
    "        3. Custom time-series splits that respect prediction horizon\n",
    "        4. Strict data isolation between train/test\n",
    "        \"\"\"\n",
    "        print(f\"  Using {n_splits} folds with minimum {min_test_size} day test periods\")\n",
    "        \n",
    "        # Calculate proper split parameters\n",
    "        total_days = len(df)\n",
    "        prediction_gap = self.predictor.prediction_horizon  # 30 days gap needed\n",
    "        min_train_size = self.predictor.sequence_length + 365  # Minimum 1 year + sequence\n",
    "        \n",
    "        # Calculate if we have enough data for proper splits\n",
    "        total_required = min_train_size + (min_test_size + prediction_gap) * n_splits\n",
    "        \n",
    "        if total_days < total_required:\n",
    "            print(f\"    Warning: Dataset too small ({total_days} < {total_required})\")\n",
    "            # Reduce parameters while maintaining data integrity\n",
    "            n_splits = max(2, min(3, total_days // (min_train_size + min_test_size + prediction_gap)))\n",
    "            min_test_size = max(200, (total_days - min_train_size - prediction_gap * n_splits) // n_splits)\n",
    "            print(f\"    Adjusted: {n_splits} splits with {min_test_size} test size\")\n",
    "        \n",
    "        # Manual time-series split with proper gaps\n",
    "        splits = []\n",
    "        \n",
    "        # Calculate split points\n",
    "        remaining_data = total_days - min_train_size\n",
    "        test_periods = []\n",
    "        \n",
    "        for i in range(n_splits):\n",
    "            # Calculate test period for this fold\n",
    "            test_start_idx = min_train_size + i * (min_test_size + prediction_gap)\n",
    "            test_end_idx = min(test_start_idx + min_test_size, total_days - prediction_gap)\n",
    "            \n",
    "            if test_end_idx - test_start_idx < 100:  # Minimum 100 test samples\n",
    "                print(f\"    Stopping at fold {i+1} - insufficient data remaining\")\n",
    "                n_splits = i\n",
    "                break\n",
    "                \n",
    "            # Training data: everything before test period (with gap)\n",
    "            train_end_idx = test_start_idx - prediction_gap  # Gap to prevent look-ahead\n",
    "            train_start_idx = 0\n",
    "            \n",
    "            if train_end_idx - train_start_idx < min_train_size:\n",
    "                print(f\"    Stopping at fold {i+1} - insufficient training data\")\n",
    "                n_splits = i\n",
    "                break\n",
    "                \n",
    "            splits.append({\n",
    "                'train_start': train_start_idx,\n",
    "                'train_end': train_end_idx,\n",
    "                'test_start': test_start_idx,\n",
    "                'test_end': test_end_idx\n",
    "            })\n",
    "        \n",
    "        if n_splits == 0:\n",
    "            print(\"    âŒ Cannot create any valid splits with current data\")\n",
    "            return {\n",
    "                'fold_performance': [],\n",
    "                'aggregate_metrics': {\n",
    "                    'mean_direction_accuracy': 0.5,\n",
    "                    'std_direction_accuracy': 0,\n",
    "                    'mean_sharpe': 0,\n",
    "                    'std_sharpe': 0,\n",
    "                    'mean_max_drawdown': 0,\n",
    "                    'worst_drawdown': 0,\n",
    "                    'successful_folds': 0,\n",
    "                    'total_folds': 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        results = {\n",
    "            'fold_performance': [],\n",
    "            'predictions': [],\n",
    "            'actuals': [],\n",
    "            'periods': []\n",
    "        }\n",
    "        \n",
    "        successful_folds = 0\n",
    "        \n",
    "        for fold, split in enumerate(splits):\n",
    "            print(f\"\\n  Fold {fold+1}/{n_splits}\")\n",
    "            \n",
    "            # Extract train and test data with strict temporal separation\n",
    "            train_data = df.iloc[split['train_start']:split['train_end']].copy()\n",
    "            test_data = df.iloc[split['test_start']:split['test_end']].copy()\n",
    "            \n",
    "            train_size = len(train_data)\n",
    "            test_size = len(test_data)\n",
    "            gap_days = split['test_start'] - split['train_end']\n",
    "            \n",
    "            print(f\"    Train: {train_size} days ({train_data.index[0]} to {train_data.index[-1]})\")\n",
    "            print(f\"    Gap: {gap_days} days (prevents look-ahead bias)\")\n",
    "            print(f\"    Test: {test_size} days ({test_data.index[0]} to {test_data.index[-1]})\")\n",
    "            \n",
    "            try:\n",
    "                # Train model on training data only (no validation split to maximize training data)\n",
    "                print(\"    Training model...\")\n",
    "                self.predictor.train_ensemble(\n",
    "                    train_data, validation_split=0.15, epochs=75, batch_size=32\n",
    "                )\n",
    "                \n",
    "                # Prepare test data - CRITICAL: Only use test data for target engineering\n",
    "                # This ensures no look-ahead bias from future data\n",
    "                print(\"    Preparing test data...\")\n",
    "                \n",
    "                # Create a buffer to ensure we have enough data after target engineering\n",
    "                # We need extra data because target engineering will remove the last 30 days\n",
    "                test_buffer = test_data.copy()\n",
    "                \n",
    "                # Engineer targets on test data only\n",
    "                df_test_proc = self.predictor.engineer_30day_target(test_buffer)\n",
    "                \n",
    "                # Remove the last prediction_horizon days since they won't have valid targets\n",
    "                # This is crucial to prevent look-ahead bias\n",
    "                df_test_proc = df_test_proc.iloc[:-self.predictor.prediction_horizon]\n",
    "                \n",
    "                if len(df_test_proc) < 50:  # Need minimum 50 valid samples\n",
    "                    print(f\"    Skipping fold {fold+1} - insufficient valid test samples after target engineering\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare features and targets\n",
    "                features_test, _ = self.predictor.prepare_features(df_test_proc)\n",
    "                targets_test = df_test_proc['target_return_30d'].values\n",
    "                \n",
    "                # Remove any NaN values that might have been introduced\n",
    "                valid_mask = ~(np.isnan(targets_test) | np.isinf(targets_test))\n",
    "                features_test = features_test[valid_mask]\n",
    "                targets_test = targets_test[valid_mask]\n",
    "                \n",
    "                if len(targets_test) < 50:\n",
    "                    print(f\"    Skipping fold {fold+1} - insufficient valid targets after cleaning\")\n",
    "                    continue\n",
    "                \n",
    "                # Create sequences\n",
    "                X_test, y_test, _ = self.predictor.create_sequences(features_test, targets_test)\n",
    "                \n",
    "                print(f\"    Created {len(X_test)} valid test sequences\")\n",
    "                \n",
    "                if len(X_test) < 30:  # Require at least 30 test samples for meaningful statistics\n",
    "                    print(f\"    Skipping fold {fold+1} - too few test sequences ({len(X_test)} < 30)\")\n",
    "                    continue\n",
    "                \n",
    "                # Make predictions\n",
    "                print(\"    Making predictions...\")\n",
    "                ensemble_pred, _, _ = self.predictor.predict_ensemble(X_test)\n",
    "                \n",
    "                # Calculate metrics with proper validation\n",
    "                mae = np.mean(np.abs(y_test - ensemble_pred.flatten()))\n",
    "                direction_acc = np.mean(np.sign(y_test) == np.sign(ensemble_pred.flatten()))\n",
    "                \n",
    "                # Trading simulation metrics\n",
    "                predicted_positions = np.sign(ensemble_pred.flatten())\n",
    "                actual_returns = y_test\n",
    "                strategy_returns = predicted_positions * actual_returns\n",
    "                \n",
    "                # Robust metrics calculation with proper error handling\n",
    "                mean_return = np.mean(strategy_returns)\n",
    "                std_return = np.std(strategy_returns)\n",
    "                sharpe_ratio = mean_return / (std_return + 1e-8) * np.sqrt(252/30) if std_return > 1e-8 else 0\n",
    "                max_drawdown = self._calculate_max_drawdown(strategy_returns)\n",
    "                \n",
    "                win_rate = np.sum(strategy_returns > 0) / len(strategy_returns) if len(strategy_returns) > 0 else 0\n",
    "                \n",
    "                # Fixed profit factor calculation\n",
    "                wins = strategy_returns[strategy_returns > 0]\n",
    "                losses = strategy_returns[strategy_returns < 0]\n",
    "                profit_factor = (np.sum(wins) / (np.abs(np.sum(losses)) + 1e-8)) if len(losses) > 0 else np.inf\n",
    "                \n",
    "                # Cap unrealistic values\n",
    "                profit_factor = min(profit_factor, 10.0)  # Cap at 10x to prevent unrealistic values\n",
    "                sharpe_ratio = np.clip(sharpe_ratio, -5.0, 5.0)  # Cap Sharpe ratio\n",
    "                \n",
    "                fold_metrics = {\n",
    "                    'fold': fold + 1,\n",
    "                    'mae': float(mae),\n",
    "                    'direction_accuracy': float(direction_acc),\n",
    "                    'mean_return': float(mean_return),\n",
    "                    'std_return': float(std_return),\n",
    "                    'sharpe_ratio': float(sharpe_ratio),\n",
    "                    'max_drawdown': float(max_drawdown),\n",
    "                    'win_rate': float(win_rate),\n",
    "                    'profit_factor': float(profit_factor),\n",
    "                    'train_start': str(train_data.index[0]),\n",
    "                    'train_end': str(train_data.index[-1]),\n",
    "                    'test_start': str(test_data.index[0]),\n",
    "                    'test_end': str(test_data.index[-1]),\n",
    "                    'n_test_samples': int(len(X_test)),\n",
    "                    'gap_days': int(gap_days)\n",
    "                }\n",
    "                \n",
    "                results['fold_performance'].append(fold_metrics)\n",
    "                results['predictions'].extend(ensemble_pred.flatten().tolist())\n",
    "                results['actuals'].extend(y_test.tolist())\n",
    "                successful_folds += 1\n",
    "                \n",
    "                print(f\"    âœ… Direction Accuracy: {direction_acc:.3f} ({len(X_test)} samples)\")\n",
    "                print(f\"    âœ… Sharpe Ratio: {sharpe_ratio:.3f}\")\n",
    "                print(f\"    âœ… Max Drawdown: {max_drawdown:.3f}\")\n",
    "                print(f\"    âœ… Win Rate: {win_rate:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ Error in fold {fold+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Aggregate metrics with proper error handling\n",
    "        if results['fold_performance']:\n",
    "            perf_df = pd.DataFrame(results['fold_performance'])\n",
    "            \n",
    "            # Calculate robust aggregate metrics\n",
    "            results['aggregate_metrics'] = {\n",
    "                'mean_direction_accuracy': float(perf_df['direction_accuracy'].mean()),\n",
    "                'std_direction_accuracy': float(perf_df['direction_accuracy'].std()),\n",
    "                'mean_sharpe': float(perf_df['sharpe_ratio'].mean()),\n",
    "                'std_sharpe': float(perf_df['sharpe_ratio'].std()),\n",
    "                'mean_max_drawdown': float(perf_df['max_drawdown'].mean()),\n",
    "                'worst_drawdown': float(perf_df['max_drawdown'].min()),  # Most negative drawdown\n",
    "                'mean_win_rate': float(perf_df['win_rate'].mean()),\n",
    "                'mean_profit_factor': float(perf_df['profit_factor'].mean()),\n",
    "                'successful_folds': int(successful_folds),\n",
    "                'total_folds': int(n_splits),\n",
    "                'total_test_samples': int(perf_df['n_test_samples'].sum())\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n  âœ… Walk-forward analysis completed: {successful_folds}/{n_splits} successful folds\")\n",
    "            print(f\"  ðŸ“Š Total test samples: {perf_df['n_test_samples'].sum()}\")\n",
    "            print(f\"  ðŸ“ˆ Mean accuracy: {perf_df['direction_accuracy'].mean():.3f} Â± {perf_df['direction_accuracy'].std():.3f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n  âŒ Walk-forward analysis failed: No successful folds\")\n",
    "            results['aggregate_metrics'] = {\n",
    "                'mean_direction_accuracy': 0.5,\n",
    "                'std_direction_accuracy': 0,\n",
    "                'mean_sharpe': 0,\n",
    "                'std_sharpe': 0,\n",
    "                'mean_max_drawdown': 0,\n",
    "                'worst_drawdown': 0,\n",
    "                'mean_win_rate': 0.5,\n",
    "                'mean_profit_factor': 1.0,\n",
    "                'successful_folds': 0,\n",
    "                'total_folds': n_splits,\n",
    "                'total_test_samples': 0\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_statistical_significance(self, df, n_permutations=500):\n",
    "        \"\"\"\n",
    "        IMPROVED: Test statistical significance with better methodology\n",
    "        \"\"\"\n",
    "        print(f\"  Running statistical significance tests with {n_permutations} permutations...\")\n",
    "        \n",
    "        try:\n",
    "            # Get model predictions on a larger subset for better statistical power\n",
    "            test_fraction = 0.4  # Use 40% of data for testing\n",
    "            split_idx = int((1 - test_fraction) * len(df))\n",
    "            train_df = df.iloc[:split_idx]\n",
    "            test_df = df.iloc[split_idx:]\n",
    "            \n",
    "            print(f\"    Using {len(train_df)} days for training, {len(test_df)} days for testing\")\n",
    "            \n",
    "            # Train model specifically for this test\n",
    "            self.predictor.train_ensemble(train_df, validation_split=0.2, epochs=100, batch_size=32)\n",
    "            \n",
    "            # Get predictions on test set\n",
    "            df_test_proc = self.predictor.engineer_30day_target(test_df)\n",
    "            features_test, _ = self.predictor.prepare_features(df_test_proc)\n",
    "            targets_test = df_test_proc['target_return_30d'].values\n",
    "            \n",
    "            X_test, y_test, _ = self.predictor.create_sequences(features_test, targets_test)\n",
    "            \n",
    "            if len(X_test) < 50:\n",
    "                print(f\"    Warning: Small test set ({len(X_test)} samples)\")\n",
    "                \n",
    "            # Get predictions\n",
    "            ensemble_pred, _, _ = self.predictor.predict_ensemble(X_test)\n",
    "            \n",
    "            # Test 1: Direction accuracy vs random (binomial test)\n",
    "            direction_correct = np.sum(np.sign(y_test) == np.sign(ensemble_pred.flatten()))\n",
    "            n_samples = len(y_test)\n",
    "            direction_accuracy = direction_correct / n_samples\n",
    "            \n",
    "            print(f\"    Direction accuracy: {direction_accuracy:.3f} ({direction_correct}/{n_samples})\")\n",
    "            \n",
    "            # Fixed scipy import with better fallback\n",
    "            try:\n",
    "                from scipy.stats import binomtest\n",
    "                p_value_direction = binomtest(direction_correct, n_samples, 0.5, alternative='greater').pvalue\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    from scipy.stats import binom_test\n",
    "                    p_value_direction = binom_test(direction_correct, n_samples, 0.5, alternative='greater')\n",
    "                except ImportError:\n",
    "                    from scipy.stats import binom\n",
    "                    p_value_direction = 1 - binom.cdf(direction_correct - 1, n_samples, 0.5)\n",
    "            \n",
    "            # Test 2: Returns vs random strategy (t-test)\n",
    "            strategy_returns = np.sign(ensemble_pred.flatten()) * y_test\n",
    "            \n",
    "            # Generate multiple random baselines for better comparison\n",
    "            random_returns_collection = []\n",
    "            for _ in range(10):\n",
    "                random_positions = np.random.choice([-1, 1], size=len(y_test))\n",
    "                random_returns = random_positions * y_test\n",
    "                random_returns_collection.extend(random_returns)\n",
    "            \n",
    "            from scipy.stats import ttest_ind\n",
    "            t_stat, p_value_returns = ttest_ind(strategy_returns, random_returns_collection)\n",
    "            \n",
    "            # Test 3: IMPROVED Permutation test for robustness\n",
    "            print(f\"    Running permutation test with {n_permutations} iterations...\")\n",
    "            \n",
    "            # Calculate actual strategy performance\n",
    "            actual_sharpe = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-6)\n",
    "            actual_mean_return = np.mean(strategy_returns)\n",
    "            \n",
    "            # Generate permutation distribution\n",
    "            permuted_sharpes = []\n",
    "            permuted_returns = []\n",
    "            \n",
    "            for i in range(n_permutations):\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"\\r      Progress: {i}/{n_permutations}\", end='')\n",
    "                \n",
    "                # Create permuted predictions by shuffling the prediction signs\n",
    "                permuted_positions = np.random.permutation(np.sign(ensemble_pred.flatten()))\n",
    "                permuted_strategy_returns = permuted_positions * y_test\n",
    "                \n",
    "                # Calculate permuted metrics\n",
    "                permuted_sharpe = np.mean(permuted_strategy_returns) / (np.std(permuted_strategy_returns) + 1e-6)\n",
    "                permuted_mean = np.mean(permuted_strategy_returns)\n",
    "                \n",
    "                permuted_sharpes.append(permuted_sharpe)\n",
    "                permuted_returns.append(permuted_mean)\n",
    "            \n",
    "            print(f\"\\r      Completed {n_permutations} permutations\")\n",
    "            \n",
    "            # Calculate p-values\n",
    "            p_value_sharpe = np.sum(np.array(permuted_sharpes) >= actual_sharpe) / n_permutations\n",
    "            p_value_mean_return = np.sum(np.array(permuted_returns) >= actual_mean_return) / n_permutations\n",
    "            \n",
    "            # Use the more conservative p-value\n",
    "            p_value_permutation = max(p_value_sharpe, p_value_mean_return)\n",
    "            \n",
    "            print(f\"    Actual Sharpe: {actual_sharpe:.3f}\")\n",
    "            print(f\"    P-value (direction): {p_value_direction:.4f}\")\n",
    "            print(f\"    P-value (permutation): {p_value_permutation:.4f}\")\n",
    "            \n",
    "            results = {\n",
    "                'n_samples': n_samples,\n",
    "                'direction_accuracy': direction_accuracy,\n",
    "                'p_value_direction': p_value_direction,\n",
    "                'mean_strategy_return': actual_mean_return,\n",
    "                'mean_random_return': np.mean(random_returns_collection),\n",
    "                'p_value_returns': p_value_returns,\n",
    "                'actual_sharpe': actual_sharpe,\n",
    "                'p_value_permutation': p_value_permutation,\n",
    "                'is_significant_alpha_05': p_value_direction < 0.05 and p_value_permutation < 0.05,\n",
    "                'is_significant_alpha_01': p_value_direction < 0.01 and p_value_permutation < 0.01\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error in statistical significance test: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'is_significant_alpha_05': False,\n",
    "                'is_significant_alpha_01': False,\n",
    "                'n_samples': 0,\n",
    "                'direction_accuracy': 0.5,\n",
    "                'p_value_direction': 1.0,\n",
    "                'p_value_permutation': 1.0,\n",
    "                'mean_strategy_return': 0,\n",
    "                'mean_random_return': 0,\n",
    "                'p_value_returns': 1.0,\n",
    "                'actual_sharpe': 0\n",
    "            }\n",
    "    \n",
    "    def calculate_risk_metrics(self, df):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive risk-adjusted performance metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get predictions on recent data\n",
    "            test_size = min(365, len(df) // 5)  # Last year or 20% of data\n",
    "            test_df = df.iloc[-test_size:]\n",
    "            \n",
    "            df_proc = self.predictor.engineer_30day_target(test_df)\n",
    "            if len(df_proc) < self.predictor.sequence_length + 30:\n",
    "                print(\"  Warning: Insufficient data for risk metrics\")\n",
    "                return {\n",
    "                    'error': 'Insufficient data',\n",
    "                    'sharpe_ratio': 0, 'sortino_ratio': 0, 'max_drawdown': 0, 'profit_factor': 1.0, \n",
    "                    'win_rate': 0.5, 'var_95': 0, 'cvar_95': 0, 'total_return': 0, 'mean_return': 0, \n",
    "                    'std_return': 0, 'calmar_ratio': 0, 'avg_win': 0, 'avg_loss': 0, 'var_99': 0, \n",
    "                    'cvar_99': 0, 'risk_adjusted_return': 0\n",
    "                }\n",
    "                \n",
    "            features, _ = self.predictor.prepare_features(df_proc)\n",
    "            targets = df_proc['target_return_30d'].values\n",
    "            \n",
    "            X, y, _ = self.predictor.create_sequences(features, targets)\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                print(\"  Warning: No sequences created for risk metrics\")\n",
    "                return {\n",
    "                    'error': 'No sequences created',\n",
    "                    'sharpe_ratio': 0, 'sortino_ratio': 0, 'max_drawdown': 0, 'profit_factor': 1.0, \n",
    "                    'win_rate': 0.5, 'var_95': 0, 'cvar_95': 0, 'total_return': 0, 'mean_return': 0, \n",
    "                    'std_return': 0, 'calmar_ratio': 0, 'avg_win': 0, 'avg_loss': 0, 'var_99': 0, \n",
    "                    'cvar_99': 0, 'risk_adjusted_return': 0\n",
    "                }\n",
    "            \n",
    "            # Use the already-trained model for predictions\n",
    "            ensemble_pred, _, _ = self.predictor.predict_ensemble(X)\n",
    "            \n",
    "            # FIXED: Calculate safe position sizes instead of full positions\n",
    "            # Old dangerous code:\n",
    "            # positions = np.sign(ensemble_pred.flatten())\n",
    "            # returns = positions * y\n",
    "            \n",
    "            # New safe position sizing:\n",
    "            raw_predictions = ensemble_pred.flatten()\n",
    "            \n",
    "            # Calculate position sizes with maximum 2% portfolio risk\n",
    "            position_sizes = []\n",
    "            for pred in raw_predictions:\n",
    "                # Get direction\n",
    "                direction = np.sign(pred)\n",
    "                # Get confidence (absolute value of prediction)\n",
    "                confidence = abs(pred)\n",
    "                # Cap position size at 2% of portfolio\n",
    "                position_size = direction * min(0.02, confidence * 0.05)\n",
    "                position_sizes.append(position_size)\n",
    "            \n",
    "            position_sizes = np.array(position_sizes)\n",
    "            \n",
    "            # Calculate returns with safe position sizing\n",
    "            returns = position_sizes * y\n",
    "            \n",
    "            # Basic metrics\n",
    "            total_return = np.sum(returns)\n",
    "            mean_return = np.mean(returns)\n",
    "            std_return = np.std(returns)\n",
    "            \n",
    "            # Sharpe ratio (annualized for 30-day returns)\n",
    "            sharpe_ratio = mean_return / (std_return + 1e-6) * np.sqrt(252/30)\n",
    "            \n",
    "            # Sortino ratio (downside deviation)\n",
    "            downside_returns = returns[returns < 0]\n",
    "            downside_std = np.std(downside_returns) if len(downside_returns) > 0 else 1e-6\n",
    "            sortino_ratio = mean_return / downside_std * np.sqrt(252/30)\n",
    "            \n",
    "            # Maximum drawdown\n",
    "            cumulative_returns = np.cumprod(1 + returns)\n",
    "            running_max = np.maximum.accumulate(cumulative_returns)\n",
    "            drawdown = (cumulative_returns - running_max) / running_max\n",
    "            max_drawdown = np.min(drawdown)\n",
    "            \n",
    "            # Calmar ratio\n",
    "            calmar_ratio = mean_return * 252/30 / (abs(max_drawdown) + 1e-6)\n",
    "            \n",
    "            # Win/loss metrics\n",
    "            winning_trades = returns[returns > 0]\n",
    "            losing_trades = returns[returns < 0]\n",
    "            \n",
    "            win_rate = len(winning_trades) / len(returns) if len(returns) > 0 else 0.5\n",
    "            avg_win = np.mean(winning_trades) if len(winning_trades) > 0 else 0\n",
    "            avg_loss = np.mean(losing_trades) if len(losing_trades) > 0 else 0\n",
    "            profit_factor = np.sum(winning_trades) / (abs(np.sum(losing_trades)) + 1e-6)\n",
    "            \n",
    "            # Value at Risk (95% and 99%)\n",
    "            var_95 = np.percentile(returns, 5) if len(returns) > 0 else 0\n",
    "            var_99 = np.percentile(returns, 1) if len(returns) > 0 else 0\n",
    "            \n",
    "            # Conditional Value at Risk (Expected Shortfall)\n",
    "            cvar_95 = np.mean(returns[returns <= var_95]) if len(returns[returns <= var_95]) > 0 else 0\n",
    "            cvar_99 = np.mean(returns[returns <= var_99]) if len(returns[returns <= var_99]) > 0 else 0\n",
    "            \n",
    "            results = {\n",
    "                'total_return': total_return,\n",
    "                'mean_return': mean_return,\n",
    "                'std_return': std_return,\n",
    "                'sharpe_ratio': sharpe_ratio,\n",
    "                'sortino_ratio': sortino_ratio,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'calmar_ratio': calmar_ratio,\n",
    "                'win_rate': win_rate,\n",
    "                'avg_win': avg_win,\n",
    "                'avg_loss': avg_loss,\n",
    "                'profit_factor': profit_factor,\n",
    "                'var_95': var_95,\n",
    "                'var_99': var_99,\n",
    "                'cvar_95': cvar_95,\n",
    "                'cvar_99': cvar_99,\n",
    "                'risk_adjusted_return': mean_return / (abs(cvar_95) + 1e-6)\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in risk metrics calculation: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'sharpe_ratio': 0, 'sortino_ratio': 0, 'max_drawdown': 0, 'profit_factor': 1.0, \n",
    "                'win_rate': 0.5, 'var_95': 0, 'cvar_95': 0, 'total_return': 0, 'mean_return': 0, \n",
    "                'std_return': 0, 'calmar_ratio': 0, 'avg_win': 0, 'avg_loss': 0, 'var_99': 0, \n",
    "                'cvar_99': 0, 'risk_adjusted_return': 0\n",
    "            }\n",
    "    \n",
    "    def test_regime_performance(self, df):\n",
    "        \"\"\"\n",
    "        Test model performance across different market regimes\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_proc = self.predictor.engineer_30day_target(df)\n",
    "            features, _ = self.predictor.prepare_features(df_proc)\n",
    "            targets = df_proc['target_return_30d'].values\n",
    "            regimes = df_proc['market_regime'].values\n",
    "            \n",
    "            X, y, regime_seq = self.predictor.create_sequences(features, targets, regimes)\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                print(\"  Warning: No sequences created for regime analysis\")\n",
    "                return {\n",
    "                    'error': 'No sequences created',\n",
    "                    'regime_performance': {}, 'regime_stability_score': 0, \n",
    "                    'worst_regime': 'unknown', 'best_regime': 'unknown'\n",
    "                }\n",
    "            \n",
    "            # Get predictions using the pre-trained model\n",
    "            ensemble_pred, _, _ = self.predictor.predict_ensemble(X)\n",
    "            \n",
    "            # Analyze by regime\n",
    "            unique_regimes = np.unique(regime_seq)\n",
    "            regime_results = {}\n",
    "            \n",
    "            for regime in unique_regimes:\n",
    "                mask = np.array(regime_seq) == regime\n",
    "                if mask.sum() < 10:  # Skip if too few samples\n",
    "                    continue\n",
    "                    \n",
    "                regime_y = y[mask]\n",
    "                regime_pred = ensemble_pred[mask].flatten()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                direction_acc = np.mean(np.sign(regime_y) == np.sign(regime_pred))\n",
    "                mae = np.mean(np.abs(regime_y - regime_pred))\n",
    "                \n",
    "                # Trading returns\n",
    "                positions = np.sign(regime_pred)\n",
    "                returns = positions * regime_y\n",
    "                \n",
    "                regime_results[regime] = {\n",
    "                    'sample_count': mask.sum(),\n",
    "                    'direction_accuracy': direction_acc,\n",
    "                    'mae': mae,\n",
    "                    'mean_return': np.mean(returns),\n",
    "                    'std_return': np.std(returns),\n",
    "                    'sharpe_ratio': np.mean(returns) / (np.std(returns) + 1e-6) * np.sqrt(252/30),\n",
    "                    'max_drawdown': self._calculate_max_drawdown(returns),\n",
    "                    'win_rate': np.sum(returns > 0) / len(returns) if len(returns) > 0 else 0.5\n",
    "                }\n",
    "            \n",
    "            # Calculate regime stability score\n",
    "            if regime_results:\n",
    "                accuracies = [r['direction_accuracy'] for r in regime_results.values()]\n",
    "                regime_stability_score = 1 - (np.std(accuracies) / (np.mean(accuracies) + 1e-6))\n",
    "                \n",
    "                worst_regime = min(regime_results.items(), key=lambda x: x[1]['direction_accuracy'])[0]\n",
    "                best_regime = max(regime_results.items(), key=lambda x: x[1]['direction_accuracy'])[0]\n",
    "            else:\n",
    "                regime_stability_score = 0\n",
    "                worst_regime = 'unknown'\n",
    "                best_regime = 'unknown'\n",
    "            \n",
    "            results = {\n",
    "                'regime_performance': regime_results,\n",
    "                'regime_stability_score': regime_stability_score,\n",
    "                'worst_regime': worst_regime,\n",
    "                'best_regime': best_regime\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in regime performance test: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'regime_performance': {}, 'regime_stability_score': 0, \n",
    "                'worst_regime': 'unknown', 'best_regime': 'unknown'\n",
    "            }\n",
    "    \n",
    "    def test_prediction_stability(self, df, n_runs=3):\n",
    "        \"\"\"\n",
    "        IMPROVED: Test consistency of predictions across multiple training runs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use larger test set for better stability assessment\n",
    "            test_size = min(100, len(df) // 15)\n",
    "            test_df = df.iloc[-test_size-self.predictor.sequence_length-30:]\n",
    "            \n",
    "            if len(test_df) < self.predictor.sequence_length + 30:\n",
    "                print(\"  Warning: Insufficient data for stability test\")\n",
    "                return {\n",
    "                    'error': 'Insufficient data',\n",
    "                    'mean_direction_agreement': 0.5, 'mean_correlation_between_runs': 0.5, \n",
    "                    'is_stable': False, 'mean_prediction_std': 0, 'max_prediction_std': 0, \n",
    "                    'min_direction_agreement': 0, 'min_correlation_between_runs': 0\n",
    "                }\n",
    "            \n",
    "            df_proc = self.predictor.engineer_30day_target(test_df)\n",
    "            features, _ = self.predictor.prepare_features(df_proc)\n",
    "            \n",
    "            X_all, _, _ = self.predictor.create_sequences(\n",
    "                features, \n",
    "                df_proc['target_return_30d'].values\n",
    "            )\n",
    "            \n",
    "            if len(X_all) == 0:\n",
    "                print(\"  Warning: No sequences created for stability test\")\n",
    "                return {\n",
    "                    'error': 'No sequences created',\n",
    "                    'mean_direction_agreement': 0.5, 'mean_correlation_between_runs': 0.5, \n",
    "                    'is_stable': False, 'mean_prediction_std': 0, 'max_prediction_std': 0, \n",
    "                    'min_direction_agreement': 0, 'min_correlation_between_runs': 0\n",
    "                }\n",
    "            \n",
    "            X_test = X_all[-min(test_size, len(X_all)):]\n",
    "            \n",
    "            # Get predictions from multiple runs with different seeds\n",
    "            all_predictions = []\n",
    "            all_directions = []\n",
    "            \n",
    "            for run in range(n_runs):\n",
    "                print(f\"\\r    Stability test run {run+1}/{n_runs}\", end='')\n",
    "                \n",
    "                # Set different random seeds for reproducibility\n",
    "                np.random.seed(run * 42 + 123)\n",
    "                tf.random.set_seed(run * 42 + 123)\n",
    "                \n",
    "                # Retrain model with consistent parameters but different initialization\n",
    "                train_df = df.iloc[:-test_size] if test_size < len(df) else df.iloc[:-10]\n",
    "                \n",
    "                # Use more epochs and consistent training for better stability\n",
    "                self.predictor.train_ensemble(train_df, epochs=50, batch_size=32)\n",
    "                \n",
    "                # Get predictions\n",
    "                pred, _, _ = self.predictor.predict_ensemble(X_test)\n",
    "                all_predictions.append(pred.flatten())\n",
    "                all_directions.append(np.sign(pred.flatten()))\n",
    "            \n",
    "            print()  # New line after progress\n",
    "            \n",
    "            # Calculate stability metrics\n",
    "            pred_array = np.array(all_predictions)\n",
    "            dir_array = np.array(all_directions)\n",
    "            \n",
    "            # Standard deviation of predictions\n",
    "            pred_std = np.std(pred_array, axis=0)\n",
    "            mean_pred_std = np.mean(pred_std)\n",
    "            \n",
    "            # Direction agreement (fraction of samples where all runs agree)\n",
    "            direction_agreement = []\n",
    "            for i in range(len(X_test)):\n",
    "                unique_dirs = np.unique(dir_array[:, i])\n",
    "                agreement = 1.0 if len(unique_dirs) == 1 else 0.0\n",
    "                direction_agreement.append(agreement)\n",
    "            \n",
    "            mean_direction_agreement = np.mean(direction_agreement)\n",
    "            \n",
    "            # Correlation between runs\n",
    "            correlations = []\n",
    "            for i in range(n_runs):\n",
    "                for j in range(i+1, n_runs):\n",
    "                    corr = np.corrcoef(pred_array[i], pred_array[j])[0, 1]\n",
    "                    if not np.isnan(corr):\n",
    "                        correlations.append(corr)\n",
    "            \n",
    "            mean_correlation = np.mean(correlations) if correlations else 0.5\n",
    "            \n",
    "            # Improved stability criteria\n",
    "            results = {\n",
    "                'mean_prediction_std': mean_pred_std,\n",
    "                'max_prediction_std': np.max(pred_std),\n",
    "                'mean_direction_agreement': mean_direction_agreement,\n",
    "                'min_direction_agreement': np.min(direction_agreement) if direction_agreement else 0,\n",
    "                'mean_correlation_between_runs': mean_correlation,\n",
    "                'min_correlation_between_runs': np.min(correlations) if correlations else 0,\n",
    "                'is_stable': mean_direction_agreement > 0.7 and mean_correlation > 0.7  # More stringent criteria\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in stability test: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'mean_direction_agreement': 0.5, 'mean_correlation_between_runs': 0.5, \n",
    "                'is_stable': False, 'mean_prediction_std': 0, 'max_prediction_std': 0, \n",
    "                'min_direction_agreement': 0, 'min_correlation_between_runs': 0\n",
    "            }\n",
    "    \n",
    "    def analyze_feature_importance(self, df, n_iterations=3):\n",
    "        \"\"\"\n",
    "        Analyze feature importance stability and relevance\n",
    "        \"\"\"\n",
    "        try:\n",
    "            importance_runs = []\n",
    "            \n",
    "            for i in range(n_iterations):\n",
    "                print(f\"\\r  Feature importance iteration {i+1}/{n_iterations}\", end='')\n",
    "                \n",
    "                # Train model with fewer epochs\n",
    "                self.predictor.train_ensemble(df, epochs=20, batch_size=32)\n",
    "                \n",
    "                # Get feature importance from Random Forest\n",
    "                if 'random_forest' in self.predictor.models:\n",
    "                    rf_model = self.predictor.models['random_forest']\n",
    "                    importance_runs.append(rf_model.feature_importances_)\n",
    "            \n",
    "            print()  # New line\n",
    "            \n",
    "            if not importance_runs:\n",
    "                print(\"  Warning: No feature importance data available\")\n",
    "                return {\n",
    "                    'error': 'No Random Forest model available',\n",
    "                    'feature_stability_score': 0, 'top_20_features': [], 'top_20_importance': [], \n",
    "                    'top_20_cv': [], 'most_stable_features': [], 'unstable_features': []\n",
    "                }\n",
    "            \n",
    "            # Calculate stability metrics\n",
    "            importance_array = np.array(importance_runs)\n",
    "            mean_importance = np.mean(importance_array, axis=0)\n",
    "            std_importance = np.std(importance_array, axis=0)\n",
    "            cv_importance = std_importance / (mean_importance + 1e-10)\n",
    "            \n",
    "            # Get top features\n",
    "            n_features = min(20, len(mean_importance))\n",
    "            top_indices = np.argsort(mean_importance)[-n_features:][::-1]\n",
    "            \n",
    "            # Calculate feature stability score\n",
    "            top_features_cv = cv_importance[top_indices]\n",
    "            feature_stability_score = 1 - np.mean(top_features_cv)\n",
    "            \n",
    "            results = {\n",
    "                'top_20_features': top_indices.tolist(),\n",
    "                'top_20_importance': mean_importance[top_indices].tolist(),\n",
    "                'top_20_cv': cv_importance[top_indices].tolist(),\n",
    "                'feature_stability_score': feature_stability_score,\n",
    "                'most_stable_features': np.where(cv_importance < 0.2)[0].tolist(),\n",
    "                'unstable_features': np.where(cv_importance > 0.5)[0].tolist()\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in feature importance analysis: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'feature_stability_score': 0, 'top_20_features': [], 'top_20_importance': [], \n",
    "                'top_20_cv': [], 'most_stable_features': [], 'unstable_features': []\n",
    "            }\n",
    "    \n",
    "    def simulate_trading(self, df, initial_capital=10000, transaction_cost=0.001):\n",
    "        \"\"\"\n",
    "        Simulate realistic trading with transaction costs, position sizing, and stop losses\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare data\n",
    "            df_proc = self.predictor.engineer_30day_target(df)\n",
    "            features, _ = self.predictor.prepare_features(df_proc)\n",
    "            targets = df_proc['target_return_30d'].values\n",
    "            \n",
    "            X, y, _ = self.predictor.create_sequences(features, targets)\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                print(\"  Warning: No sequences created for trading simulation\")\n",
    "                return {\n",
    "                    'error': 'No sequences created',\n",
    "                    'profitable': False, 'meets_sharpe_threshold': False, 'meets_drawdown_threshold': False,\n",
    "                    'total_return': 0, 'sharpe_ratio': 0, 'max_drawdown': 0, 'n_trades': 0, 'win_rate': 0,\n",
    "                    'initial_capital': initial_capital, 'final_capital': initial_capital, \n",
    "                    'annualized_return': 0, 'avg_trade_return': 0, 'trade_frequency': 0\n",
    "                }\n",
    "            \n",
    "            # Split data\n",
    "            split_idx = int(0.7 * len(X))\n",
    "            X_train = X[:split_idx]\n",
    "            y_train = y[:split_idx]\n",
    "            X_test = X[split_idx:]\n",
    "            y_test = y[split_idx:]\n",
    "            \n",
    "            if len(X_test) == 0:\n",
    "                print(\"  Warning: No test data for trading simulation\")\n",
    "                return {\n",
    "                    'error': 'No test data',\n",
    "                    'profitable': False, 'meets_sharpe_threshold': False, 'meets_drawdown_threshold': False,\n",
    "                    'total_return': 0, 'sharpe_ratio': 0, 'max_drawdown': 0, 'n_trades': 0, 'win_rate': 0,\n",
    "                    'initial_capital': initial_capital, 'final_capital': initial_capital, \n",
    "                    'annualized_return': 0, 'avg_trade_return': 0, 'trade_frequency': 0\n",
    "                }\n",
    "            \n",
    "            # Get predictions using pre-trained model\n",
    "            ensemble_pred, _, _ = self.predictor.predict_ensemble(X_test)\n",
    "            \n",
    "            # NEW: Stop loss parameters\n",
    "            stop_loss_pct = 0.05  # 5% stop loss\n",
    "            \n",
    "            def apply_stop_loss(predicted_return, actual_return, position_type):\n",
    "                \"\"\"Apply stop loss logic to limit downside\"\"\"\n",
    "                if position_type == 'long' and actual_return < -stop_loss_pct:\n",
    "                    return -stop_loss_pct  # Cap loss at stop loss level\n",
    "                elif position_type == 'short' and actual_return > stop_loss_pct:\n",
    "                    return -stop_loss_pct  # Cap loss for short position\n",
    "                else:\n",
    "                    return actual_return  # No stop triggered, use actual return\n",
    "            \n",
    "            # Simulate trading\n",
    "            capital = initial_capital\n",
    "            positions = []\n",
    "            returns = []\n",
    "            equity_curve = [capital]\n",
    "            \n",
    "            for i in range(len(ensemble_pred)):\n",
    "                # Get prediction\n",
    "                pred_return = ensemble_pred[i][0]\n",
    "                actual_return = y_test[i]\n",
    "                \n",
    "                # Position sizing based on confidence (Kelly criterion approximation)\n",
    "                confidence = min(abs(pred_return), 0.1)  # Cap at 10% position\n",
    "                position_size = confidence\n",
    "                \n",
    "                # Determine trade\n",
    "                if abs(pred_return) > 0.02:  # Only trade if predicted return > 2%\n",
    "                    if pred_return > 0:\n",
    "                        # Long position\n",
    "                        position_value = capital * position_size\n",
    "                        # Account for transaction costs\n",
    "                        position_value *= (1 - transaction_cost)\n",
    "                        \n",
    "                        # NEW: Apply stop loss logic\n",
    "                        risk_adjusted_return = apply_stop_loss(pred_return, actual_return, 'long')\n",
    "                        \n",
    "                        # Calculate return with stop loss\n",
    "                        trade_return = position_value * risk_adjusted_return\n",
    "                        capital += trade_return - (position_value * transaction_cost)  # Exit cost\n",
    "                    else:\n",
    "                        # Short position\n",
    "                        position_value = capital * position_size\n",
    "                        position_value *= (1 - transaction_cost)\n",
    "                        \n",
    "                        # NEW: Apply stop loss logic (negative actual return for short)\n",
    "                        risk_adjusted_return = apply_stop_loss(pred_return, actual_return, 'short')\n",
    "                        \n",
    "                        trade_return = -position_value * risk_adjusted_return\n",
    "                        capital += trade_return - (position_value * transaction_cost)\n",
    "                    \n",
    "                    positions.append(np.sign(pred_return))\n",
    "                    returns.append(trade_return / (capital - trade_return) if capital - trade_return != 0 else 0)\n",
    "                else:\n",
    "                    positions.append(0)\n",
    "                    returns.append(0)\n",
    "                \n",
    "                equity_curve.append(capital)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            total_return = (capital - initial_capital) / initial_capital\n",
    "            returns_array = np.array(returns)\n",
    "            \n",
    "            # Remove zero returns for some metrics\n",
    "            active_returns = returns_array[returns_array != 0]\n",
    "            \n",
    "            if len(active_returns) > 0:\n",
    "                sharpe = np.mean(active_returns) / (np.std(active_returns) + 1e-6) * np.sqrt(252/30)\n",
    "                win_rate = np.sum(active_returns > 0) / len(active_returns)\n",
    "                avg_trade_return = np.mean(active_returns)\n",
    "            else:\n",
    "                sharpe = 0\n",
    "                win_rate = 0.5\n",
    "                avg_trade_return = 0\n",
    "            \n",
    "            # Drawdown calculation\n",
    "            equity_array = np.array(equity_curve)\n",
    "            running_max = np.maximum.accumulate(equity_array)\n",
    "            drawdown = (equity_array - running_max) / running_max\n",
    "            max_drawdown = np.min(drawdown)\n",
    "            \n",
    "            # Trade statistics\n",
    "            n_trades = np.sum(np.array(positions) != 0)\n",
    "            \n",
    "            results = {\n",
    "                'initial_capital': initial_capital,\n",
    "                'final_capital': capital,\n",
    "                'total_return': total_return,\n",
    "                'annualized_return': total_return * 252/30 / len(y_test) if len(y_test) > 0 else 0,\n",
    "                'sharpe_ratio': sharpe,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'n_trades': n_trades,\n",
    "                'win_rate': win_rate,\n",
    "                'avg_trade_return': avg_trade_return,\n",
    "                'trade_frequency': n_trades / len(y_test) if len(y_test) > 0 else 0,\n",
    "                'profitable': capital > initial_capital,\n",
    "                'meets_sharpe_threshold': sharpe > self.min_acceptable_sharpe,\n",
    "                'meets_drawdown_threshold': abs(max_drawdown) < self.max_acceptable_drawdown\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in trading simulation: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'profitable': False, 'meets_sharpe_threshold': False, 'meets_drawdown_threshold': False,\n",
    "                'total_return': 0, 'sharpe_ratio': 0, 'max_drawdown': 0, 'n_trades': 0, 'win_rate': 0,\n",
    "                'initial_capital': initial_capital, 'final_capital': initial_capital, \n",
    "                'annualized_return': 0, 'avg_trade_return': 0, 'trade_frequency': 0\n",
    "            }\n",
    "\n",
    "    def stress_test_model(self, df):\n",
    "        \"\"\"\n",
    "        Enhanced stress test with proper regime transition analysis and robust scoring\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_proc = self.predictor.engineer_30day_target(df)\n",
    "            \n",
    "            results = {\n",
    "                'extreme_volatility': {},\n",
    "                'black_swan': {},\n",
    "                'regime_transitions': {},\n",
    "                'stress_test_score': 0,\n",
    "                'passes_stress_test': False\n",
    "            }\n",
    "            \n",
    "            # Test 1: Performance during extreme volatility\n",
    "            if 'extreme_condition' in df_proc.columns:\n",
    "                extreme_vol_mask = df_proc['extreme_condition']\n",
    "                normal_mask = ~extreme_vol_mask\n",
    "                \n",
    "                for condition, mask in [('extreme', extreme_vol_mask), ('normal', normal_mask)]:\n",
    "                    if mask.sum() < self.predictor.sequence_length + 30:\n",
    "                        print(f\"  Warning: Not enough {condition} samples ({mask.sum()})\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Get aligned data for this condition\n",
    "                        condition_indices = df_proc.index[mask]\n",
    "                        condition_df = df.loc[condition_indices]\n",
    "                        \n",
    "                        if len(condition_df) < 100:\n",
    "                            print(f\"  Warning: Insufficient {condition} data ({len(condition_df)} samples)\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Process condition-specific data\n",
    "                        df_cond_proc = self.predictor.engineer_30day_target(condition_df)\n",
    "                        \n",
    "                        # Ensure we have valid targets\n",
    "                        valid_target_mask = ~df_cond_proc['target_return_30d'].isna()\n",
    "                        if valid_target_mask.sum() < 50:\n",
    "                            print(f\"  Warning: Not enough valid targets for {condition}\")\n",
    "                            continue\n",
    "                        \n",
    "                        df_cond_proc = df_cond_proc[valid_target_mask]\n",
    "                        \n",
    "                        features, _ = self.predictor.prepare_features(df_cond_proc)\n",
    "                        targets = df_cond_proc['target_return_30d'].values\n",
    "                        \n",
    "                        X, y, _ = self.predictor.create_sequences(features, targets)\n",
    "                        \n",
    "                        if len(X) > 10:  # Minimum samples for meaningful test\n",
    "                            pred, _, _ = self.predictor.predict_ensemble(X)\n",
    "                            \n",
    "                            # Clean predictions\n",
    "                            pred_flat = pred.flatten()\n",
    "                            valid_pred_mask = ~(np.isnan(pred_flat) | np.isnan(y))\n",
    "                            \n",
    "                            if valid_pred_mask.sum() > 0:\n",
    "                                y_clean = y[valid_pred_mask]\n",
    "                                pred_clean = pred_flat[valid_pred_mask]\n",
    "                                \n",
    "                                direction_acc = np.mean(np.sign(y_clean) == np.sign(pred_clean))\n",
    "                                mae = np.mean(np.abs(y_clean - pred_clean))\n",
    "                                \n",
    "                                results['extreme_volatility'][condition] = {\n",
    "                                    'direction_accuracy': float(direction_acc),\n",
    "                                    'mae': float(mae),\n",
    "                                    'sample_count': int(len(pred_clean))\n",
    "                                }\n",
    "                                \n",
    "                                print(f\"  {condition.capitalize()} conditions: {direction_acc:.3f} accuracy, {len(pred_clean)} samples\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"  Warning: Could not test {condition} conditions: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Test 2: Black swan events (improved detection)\n",
    "            if 'returns_7d' in df_proc.columns:\n",
    "                returns = df_proc['returns_7d'].dropna()\n",
    "                if len(returns) > 0:\n",
    "                    # Use both absolute threshold and rolling quantile approach\n",
    "                    rolling_std = returns.rolling(window=252, min_periods=50).std()\n",
    "                    static_threshold = 3 * returns.std()\n",
    "                    \n",
    "                    # Dynamic threshold based on rolling volatility\n",
    "                    dynamic_threshold = 2.5 * rolling_std\n",
    "                    \n",
    "                    black_swan_mask = (np.abs(returns) > static_threshold) | \\\n",
    "                                    (np.abs(returns) > dynamic_threshold)\n",
    "                    \n",
    "                    results['black_swan'] = {\n",
    "                        'n_events': str(int(black_swan_mask.sum())),\n",
    "                        'pct_of_data': float(black_swan_mask.sum() / len(returns))\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  Black swan events: {black_swan_mask.sum()} ({100*black_swan_mask.sum()/len(returns):.2f}%)\")\n",
    "            \n",
    "            # Test 3: Regime transition analysis (FIXED)\n",
    "            if 'market_regime' in df_proc.columns:\n",
    "                regimes = df_proc['market_regime'].dropna()\n",
    "                \n",
    "                if len(regimes) > 100:\n",
    "                    # Detect regime changes\n",
    "                    regime_changes = regimes != regimes.shift(1)\n",
    "                    transition_indices = np.where(regime_changes)[0]\n",
    "                    \n",
    "                    print(f\"  Found {len(transition_indices)} regime transitions\")\n",
    "                    \n",
    "                    transition_accuracies = []\n",
    "                    valid_transitions = 0\n",
    "                    \n",
    "                    for change_idx in transition_indices:\n",
    "                        # Define transition window (wider window for better analysis)\n",
    "                        window_start = max(0, change_idx - 15)\n",
    "                        window_end = min(len(df_proc), change_idx + 15)\n",
    "                        \n",
    "                        # Ensure we have enough data for prediction\n",
    "                        if window_end - window_start < self.predictor.sequence_length + 30:\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            # Get transition period data\n",
    "                            transition_slice = slice(window_start, window_end)\n",
    "                            transition_indices_actual = df_proc.iloc[transition_slice].index\n",
    "                            transition_df = df.loc[transition_indices_actual]\n",
    "                            \n",
    "                            if len(transition_df) < 50:\n",
    "                                continue\n",
    "                            \n",
    "                            # Process transition data\n",
    "                            df_trans_proc = self.predictor.engineer_30day_target(transition_df)\n",
    "                            \n",
    "                            # Check for valid targets\n",
    "                            valid_targets = ~df_trans_proc['target_return_30d'].isna()\n",
    "                            \n",
    "                            if valid_targets.sum() < 20:\n",
    "                                continue\n",
    "                            \n",
    "                            df_trans_proc = df_trans_proc[valid_targets]\n",
    "                            \n",
    "                            features, _ = self.predictor.prepare_features(df_trans_proc)\n",
    "                            targets = df_trans_proc['target_return_30d'].values\n",
    "                            \n",
    "                            X, y, _ = self.predictor.create_sequences(features, targets)\n",
    "                            \n",
    "                            if len(X) > 5:  # Minimum for transition analysis\n",
    "                                pred, _, _ = self.predictor.predict_ensemble(X)\n",
    "                                pred_flat = pred.flatten()\n",
    "                                \n",
    "                                # Clean data\n",
    "                                valid_mask = ~(np.isnan(pred_flat) | np.isnan(y))\n",
    "                                if valid_mask.sum() > 0:\n",
    "                                    y_clean = y[valid_mask]\n",
    "                                    pred_clean = pred_flat[valid_mask]\n",
    "                                    \n",
    "                                    direction_acc = np.mean(np.sign(y_clean) == np.sign(pred_clean))\n",
    "                                    transition_accuracies.append(direction_acc)\n",
    "                                    valid_transitions += 1\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"    Warning: Error processing transition at {change_idx}: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Calculate average transition accuracy\n",
    "                    if transition_accuracies:\n",
    "                        avg_transition_accuracy = np.mean(transition_accuracies)\n",
    "                        print(f\"  Average transition accuracy: {avg_transition_accuracy:.3f} ({valid_transitions} valid transitions)\")\n",
    "                    else:\n",
    "                        avg_transition_accuracy = 0\n",
    "                        print(\"  No valid transition periods found\")\n",
    "                    \n",
    "                    results['regime_transitions'] = {\n",
    "                        'n_transitions': len(transition_indices),\n",
    "                        'avg_accuracy_during_transition': float(avg_transition_accuracy)\n",
    "                    }\n",
    "            \n",
    "            # Enhanced stress test scoring\n",
    "            stress_components = []\n",
    "            \n",
    "            # Component 1: Extreme volatility performance\n",
    "            extreme_vol_score = 0.5  # Default neutral score\n",
    "            \n",
    "            if 'extreme' in results['extreme_volatility'] and 'normal' in results['extreme_volatility']:\n",
    "                extreme_acc = results['extreme_volatility']['extreme']['direction_accuracy']\n",
    "                normal_acc = results['extreme_volatility']['normal']['direction_accuracy']\n",
    "                \n",
    "                # Score based on relative performance\n",
    "                if normal_acc > 0.45:  # Only penalize if normal performance is reasonable\n",
    "                    relative_performance = extreme_acc / normal_acc\n",
    "                    extreme_vol_score = min(1.0, max(0.0, relative_performance))\n",
    "                else:\n",
    "                    extreme_vol_score = extreme_acc  # Use absolute performance if normal is poor\n",
    "            \n",
    "            stress_components.append(('extreme_volatility', extreme_vol_score, 0.4))\n",
    "            \n",
    "            # Component 2: Regime transition performance\n",
    "            transition_score = 0.3  # Default low score\n",
    "            \n",
    "            if 'avg_accuracy_during_transition' in results.get('regime_transitions', {}):\n",
    "                transition_acc = results['regime_transitions']['avg_accuracy_during_transition']\n",
    "                # Score transitions more leniently (0.4 = good, 0.5+ = excellent)\n",
    "                transition_score = min(1.0, max(0.0, transition_acc / 0.45))\n",
    "            \n",
    "            stress_components.append(('regime_transitions', transition_score, 0.4))\n",
    "            \n",
    "            # Component 3: Black swan resilience\n",
    "            black_swan_score = 0.8  # Default good score\n",
    "            \n",
    "            if 'pct_of_data' in results.get('black_swan', {}):\n",
    "                black_swan_pct = results['black_swan']['pct_of_data']\n",
    "                # Penalize if too many events are classified as black swans\n",
    "                if black_swan_pct > 0.05:  # More than 5% seems excessive\n",
    "                    black_swan_score = max(0.3, 1.0 - (black_swan_pct - 0.05) * 10)\n",
    "            \n",
    "            stress_components.append(('black_swan', black_swan_score, 0.2))\n",
    "            \n",
    "            # Calculate weighted stress score\n",
    "            total_weight = sum(weight for _, _, weight in stress_components)\n",
    "            stress_score = sum(score * weight for _, score, weight in stress_components) / total_weight\n",
    "            \n",
    "            results['stress_test_score'] = max(0.0, min(1.0, stress_score))\n",
    "            results['passes_stress_test'] = str(stress_score > 0.7)\n",
    "            \n",
    "            # Debug output\n",
    "            print(f\"  Stress test components:\")\n",
    "            for name, score, weight in stress_components:\n",
    "                print(f\"    {name}: {score:.3f} (weight: {weight})\")\n",
    "            print(f\"  Overall stress score: {stress_score:.3f}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in stress test: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'stress_test_score': 0,\n",
    "                'passes_stress_test': 'False',\n",
    "                'extreme_volatility': {},\n",
    "                'black_swan': {},\n",
    "                'regime_transitions': {}\n",
    "            }\n",
    "        \n",
    "    def _calculate_max_drawdown(self, returns):\n",
    "        \"\"\"Calculate maximum drawdown from returns series\"\"\"\n",
    "        if len(returns) == 0:\n",
    "            return 0\n",
    "        cumulative = np.cumprod(1 + returns)\n",
    "        running_max = np.maximum.accumulate(cumulative)\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        return np.min(drawdown)\n",
    "    \n",
    "    def generate_trading_readiness_report(self, save_report=True):\n",
    "        \"\"\"\n",
    "        Generate comprehensive trading readinesxs report\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRADING READINESS ASSESSMENT REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Overall readiness scores\n",
    "        readiness_scores = {}\n",
    "        \n",
    "        # 1. Performance Score\n",
    "        if 'walk_forward' in self.test_results and 'aggregate_metrics' in self.test_results['walk_forward']:\n",
    "            wf = self.test_results['walk_forward']['aggregate_metrics']\n",
    "            perf_score = 0\n",
    "            perf_score += 0.3 * min(wf.get('mean_direction_accuracy', 0.5) / 0.6, 1.0)  # Target 60% accuracy\n",
    "            perf_score += 0.3 * min(wf.get('mean_sharpe', 0) / 1.0, 1.0)  # Target Sharpe > 1\n",
    "            perf_score += 0.2 * (1 - min(abs(wf.get('worst_drawdown', 0)) / 0.2, 1.0))  # Max 20% drawdown\n",
    "            perf_score += 0.2 * (1 - min(wf.get('std_direction_accuracy', 0.1) / 0.1, 1.0))  # Low variance\n",
    "            readiness_scores['Performance'] = perf_score\n",
    "        else:\n",
    "            readiness_scores['Performance'] = 0\n",
    "        \n",
    "        # 2. Statistical Significance Score\n",
    "        if 'statistical_significance' in self.test_results:\n",
    "            sig = self.test_results['statistical_significance']\n",
    "            sig_score = 0\n",
    "            sig_score += 0.5 if sig.get('is_significant_alpha_05', False) else 0\n",
    "            sig_score += 0.5 if sig.get('is_significant_alpha_01', False) else 0.25\n",
    "            readiness_scores['Statistical_Significance'] = sig_score\n",
    "        else:\n",
    "            readiness_scores['Statistical_Significance'] = 0\n",
    "        \n",
    "        # 3. Risk Management Score\n",
    "        if 'risk_metrics' in self.test_results:\n",
    "            risk = self.test_results['risk_metrics']\n",
    "            risk_score = 0\n",
    "            risk_score += 0.25 * min(risk.get('sharpe_ratio', 0) / 1.0, 1.0)\n",
    "            risk_score += 0.25 * min(risk.get('sortino_ratio', 0) / 1.5, 1.0)\n",
    "            risk_score += 0.25 * (1 - min(abs(risk.get('max_drawdown', 0)) / 0.2, 1.0))\n",
    "            risk_score += 0.25 * min(risk.get('profit_factor', 1.0) / 1.5, 1.0)\n",
    "            readiness_scores['Risk_Management'] = risk_score\n",
    "        else:\n",
    "            readiness_scores['Risk_Management'] = 0\n",
    "        \n",
    "        # 4. Stability Score\n",
    "        if 'prediction_stability' in self.test_results:\n",
    "            stab = self.test_results['prediction_stability']\n",
    "            stab_score = 0\n",
    "            stab_score += 0.5 * stab.get('mean_direction_agreement', 0.5)\n",
    "            stab_score += 0.5 * stab.get('mean_correlation_between_runs', 0.5)\n",
    "            readiness_scores['Stability'] = stab_score\n",
    "        else:\n",
    "            readiness_scores['Stability'] = 0\n",
    "        \n",
    "        # 5. Regime Robustness Score\n",
    "        if 'regime_analysis' in self.test_results:\n",
    "            regime = self.test_results['regime_analysis']\n",
    "            regime_score = regime.get('regime_stability_score', 0)\n",
    "            readiness_scores['Regime_Robustness'] = regime_score\n",
    "        else:\n",
    "            readiness_scores['Regime_Robustness'] = 0\n",
    "        \n",
    "        # 6. Practical Trading Score\n",
    "        if 'trading_simulation' in self.test_results:\n",
    "            trade = self.test_results['trading_simulation']\n",
    "            trade_score = 0\n",
    "            trade_score += 0.4 if trade.get('profitable', False) else 0\n",
    "            trade_score += 0.3 if trade.get('meets_sharpe_threshold', False) else 0\n",
    "            trade_score += 0.3 if trade.get('meets_drawdown_threshold', False) else 0\n",
    "            readiness_scores['Practical_Trading'] = trade_score\n",
    "        else:\n",
    "            readiness_scores['Practical_Trading'] = 0\n",
    "        \n",
    "        # Calculate overall readiness\n",
    "        overall_readiness = np.mean(list(readiness_scores.values()))\n",
    "        \n",
    "        # Print detailed report\n",
    "        print(\"\\n1. PERFORMANCE METRICS\")\n",
    "        print(\"-\" * 40)\n",
    "        if 'walk_forward' in self.test_results and 'aggregate_metrics' in self.test_results['walk_forward']:\n",
    "            wf = self.test_results['walk_forward']['aggregate_metrics']\n",
    "            print(f\"Mean Direction Accuracy: {wf.get('mean_direction_accuracy', 0.5):.3f} Â± {wf.get('std_direction_accuracy', 0):.3f}\")\n",
    "            print(f\"Mean Sharpe Ratio: {wf.get('mean_sharpe', 0):.3f} Â± {wf.get('std_sharpe', 0):.3f}\")\n",
    "            print(f\"Worst Drawdown: {wf.get('worst_drawdown', 0):.3f}\")\n",
    "            print(f\"Successful Folds: {wf.get('successful_folds', 0)}/{wf.get('total_folds', 0)}\")\n",
    "        else:\n",
    "            print(\"Walk-forward analysis not completed successfully\")\n",
    "        \n",
    "        print(\"\\n2. STATISTICAL SIGNIFICANCE\")\n",
    "        print(\"-\" * 40)\n",
    "        if 'statistical_significance' in self.test_results:\n",
    "            sig = self.test_results['statistical_significance']\n",
    "            print(f\"Direction Accuracy: {sig.get('direction_accuracy', 0.5):.3f}\")\n",
    "            print(f\"P-value (Direction): {sig.get('p_value_direction', 1.0):.4f}\")\n",
    "            print(f\"P-value (Permutation): {sig.get('p_value_permutation', 1.0):.4f}\")\n",
    "            print(f\"Statistically Significant: {'Yes' if sig.get('is_significant_alpha_05', False) else 'No'}\")\n",
    "        else:\n",
    "            print(\"Statistical significance test not completed successfully\")\n",
    "        \n",
    "        print(\"\\n3. RISK METRICS\")\n",
    "        print(\"-\" * 40)\n",
    "        if 'risk_metrics' in self.test_results:\n",
    "            risk = self.test_results['risk_metrics']\n",
    "            print(f\"Sharpe Ratio: {risk.get('sharpe_ratio', 0):.3f}\")\n",
    "            print(f\"Sortino Ratio: {risk.get('sortino_ratio', 0):.3f}\")\n",
    "            print(f\"Max Drawdown: {risk.get('max_drawdown', 0):.3f}\")\n",
    "            print(f\"Win Rate: {risk.get('win_rate', 0.5):.3f}\")\n",
    "            print(f\"Profit Factor: {risk.get('profit_factor', 1.0):.3f}\")\n",
    "            print(f\"VaR (95%): {risk.get('var_95', 0):.3f}\")\n",
    "            print(f\"CVaR (95%): {risk.get('cvar_95', 0):.3f}\")\n",
    "        else:\n",
    "            print(\"Risk metrics calculation not completed successfully\")\n",
    "        \n",
    "        print(\"\\n4. STABILITY ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        if 'prediction_stability' in self.test_results:\n",
    "            stab = self.test_results['prediction_stability']\n",
    "            print(f\"Direction Agreement: {stab.get('mean_direction_agreement', 0.5):.3f}\")\n",
    "            print(f\"Prediction Correlation: {stab.get('mean_correlation_between_runs', 0.5):.3f}\")\n",
    "            print(f\"Model is Stable: {'Yes' if stab.get('is_stable', False) else 'No'}\")\n",
    "        else:\n",
    "            print(\"Stability analysis not completed successfully\")\n",
    "        \n",
    "        print(\"\\n5. REGIME PERFORMANCE\")\n",
    "        print(\"-\" * 40)\n",
    "        if 'regime_analysis' in self.test_results:\n",
    "            regime = self.test_results['regime_analysis']\n",
    "            print(f\"Regime Stability Score: {regime.get('regime_stability_score', 0):.3f}\")\n",
    "            print(f\"Best Regime: {regime.get('best_regime', 'unknown')}\")\n",
    "            print(f\"Worst Regime: {regime.get('worst_regime', 'unknown')}\")\n",
    "            \n",
    "            regime_perf = regime.get('regime_performance', {})\n",
    "            if regime_perf:\n",
    "                print(\"\\nDetailed Regime Performance:\")\n",
    "                for reg, perf in regime_perf.items():\n",
    "                    print(f\"  {reg}: Accuracy={perf.get('direction_accuracy', 0.5):.3f}, Sharpe={perf.get('sharpe_ratio', 0):.3f}\")\n",
    "        else:\n",
    "            print(\"Regime analysis not completed successfully\")\n",
    "        \n",
    "        print(\"\\n6. TRADING SIMULATION\")\n",
    "        print(\"-\" * 40)\n",
    "        if 'trading_simulation' in self.test_results:\n",
    "            trade = self.test_results['trading_simulation']\n",
    "            print(f\"Total Return: {trade.get('total_return', 0):.2%}\")\n",
    "            print(f\"Annualized Return: {trade.get('annualized_return', 0):.2%}\")\n",
    "            print(f\"Sharpe Ratio: {trade.get('sharpe_ratio', 0):.3f}\")\n",
    "            print(f\"Max Drawdown: {trade.get('max_drawdown', 0):.3f}\")\n",
    "            print(f\"Number of Trades: {trade.get('n_trades', 0)}\")\n",
    "            print(f\"Win Rate: {trade.get('win_rate', 0.5):.3f}\")\n",
    "        else:\n",
    "            print(\"Trading simulation not completed successfully\")\n",
    "        \n",
    "        print(\"\\n7. STRESS TEST RESULTS\")\n",
    "        print(\"-\" * 40)\n",
    "        if 'stress_test' in self.test_results:\n",
    "            stress = self.test_results['stress_test']\n",
    "            print(f\"Stress Test Score: {stress.get('stress_test_score', 0):.3f}\")\n",
    "            print(f\"Passes Stress Test: {'Yes' if stress.get('passes_stress_test', False) else 'No'}\")\n",
    "            \n",
    "            extreme_vol = stress.get('extreme_volatility', {})\n",
    "            if extreme_vol:\n",
    "                print(\"\\nExtreme vs Normal Conditions:\")\n",
    "                for cond, metrics in extreme_vol.items():\n",
    "                    print(f\"  {cond}: Accuracy={metrics.get('direction_accuracy', 0.5):.3f}\")\n",
    "        else:\n",
    "            print(\"Stress test not completed successfully\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"READINESS SCORES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for category, score in readiness_scores.items():\n",
    "            status = \"âœ…\" if score >= 0.7 else \"âš ï¸\" if score >= 0.5 else \"âŒ\"\n",
    "            print(f\"{status} {category}: {score:.2f}/1.00\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"OVERALL TRADING READINESS: {overall_readiness:.2f}/1.00\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Final recommendation\n",
    "        print(\"\\nRECOMMENDATION:\")\n",
    "        if overall_readiness >= 0.8:\n",
    "            print(\"âœ… Model is READY for live trading with proper risk management\")\n",
    "            print(\"   - Start with small position sizes\")\n",
    "            print(\"   - Monitor performance closely for first 30 days\")\n",
    "            print(\"   - Set strict stop-loss rules\")\n",
    "        elif overall_readiness >= 0.6:\n",
    "            print(\"âš ï¸ Model shows POTENTIAL but needs improvements:\")\n",
    "            \n",
    "            # Specific recommendations based on weak areas\n",
    "            weak_areas = [k for k, v in readiness_scores.items() if v < 0.7]\n",
    "            for area in weak_areas:\n",
    "                if area == 'Performance':\n",
    "                    print(\"   - Improve direction accuracy or reduce prediction horizon\")\n",
    "                elif area == 'Risk_Management':\n",
    "                    print(\"   - Optimize position sizing and risk controls\")\n",
    "                elif area == 'Stability':\n",
    "                    print(\"   - Add more regularization or ensemble methods\")\n",
    "                elif area == 'Regime_Robustness':\n",
    "                    print(\"   - Train on more diverse market conditions\")\n",
    "        else:\n",
    "            print(\"âŒ Model is NOT READY for live trading\")\n",
    "            print(\"   - Continue development and testing\")\n",
    "            print(\"   - Consider fundamental strategy changes\")\n",
    "        \n",
    "        # Save detailed report if requested\n",
    "        if save_report:\n",
    "            report = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'overall_readiness': overall_readiness,\n",
    "                'readiness_scores': readiness_scores,\n",
    "                'test_results': self.test_results,\n",
    "                'recommendation': 'READY' if overall_readiness >= 0.8 else 'NOT READY'\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            filename = f\"trading_readiness_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            import json\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(report, f, indent=2, default=str)\n",
    "            print(f\"\\nDetailed report saved to: {filename}\")\n",
    "        \n",
    "        return overall_readiness, readiness_scores\n",
    "    \n",
    "    def plot_test_results(self):\n",
    "        \"\"\"\n",
    "        Create visualizations of test results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "            \n",
    "            # 1. Walk-forward performance\n",
    "            if 'walk_forward' in self.test_results and 'fold_performance' in self.test_results['walk_forward']:\n",
    "                ax = axes[0, 0]\n",
    "                wf_data = pd.DataFrame(self.test_results['walk_forward']['fold_performance'])\n",
    "                \n",
    "                if len(wf_data) > 0:\n",
    "                    x = range(len(wf_data))\n",
    "                    ax.plot(x, wf_data['direction_accuracy'], 'b-o', label='Direction Accuracy')\n",
    "                    ax.plot(x, wf_data['win_rate'], 'g-s', label='Win Rate')\n",
    "                    ax.axhline(0.5, color='r', linestyle='--', alpha=0.5)\n",
    "                    \n",
    "                    ax.set_xlabel('Fold')\n",
    "                    ax.set_ylabel('Accuracy/Win Rate')\n",
    "                    ax.set_title('Walk-Forward Performance')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'No walk-forward data', ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_title('Walk-Forward Performance')\n",
    "            else:\n",
    "                ax = axes[0, 0]\n",
    "                ax.text(0.5, 0.5, 'Walk-forward analysis failed', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title('Walk-Forward Performance')\n",
    "            \n",
    "            # 2. Risk metrics visualization\n",
    "            if 'risk_metrics' in self.test_results and 'error' not in self.test_results['risk_metrics']:\n",
    "                ax = axes[0, 1]\n",
    "                risk = self.test_results['risk_metrics']\n",
    "                \n",
    "                metrics = ['Sharpe', 'Sortino', 'Calmar']\n",
    "                values = [risk.get('sharpe_ratio', 0), risk.get('sortino_ratio', 0), risk.get('calmar_ratio', 0)]\n",
    "                \n",
    "                bars = ax.bar(metrics, values)\n",
    "                for i, (metric, value) in enumerate(zip(metrics, values)):\n",
    "                    color = 'green' if value > 1 else 'orange' if value > 0.5 else 'red'\n",
    "                    bars[i].set_color(color)\n",
    "                    ax.text(i, value + 0.05, f'{value:.2f}', ha='center')\n",
    "                \n",
    "                ax.set_ylabel('Ratio')\n",
    "                ax.set_title('Risk-Adjusted Performance Ratios')\n",
    "                ax.axhline(1.0, color='black', linestyle='--', alpha=0.5)\n",
    "            else:\n",
    "                ax = axes[0, 1]\n",
    "                ax.text(0.5, 0.5, 'Risk metrics calculation failed', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title('Risk-Adjusted Performance Ratios')\n",
    "            \n",
    "            # 3. Regime performance\n",
    "            if 'regime_analysis' in self.test_results and 'regime_performance' in self.test_results['regime_analysis']:\n",
    "                ax = axes[1, 0]\n",
    "                regime_perf = self.test_results['regime_analysis']['regime_performance']\n",
    "                \n",
    "                if regime_perf:\n",
    "                    regimes = list(regime_perf.keys())\n",
    "                    accuracies = [regime_perf[r].get('direction_accuracy', 0.5) for r in regimes]\n",
    "                    \n",
    "                    bars = ax.bar(regimes, accuracies)\n",
    "                    ax.axhline(0.5, color='r', linestyle='--', alpha=0.5)\n",
    "                    ax.set_ylabel('Direction Accuracy')\n",
    "                    ax.set_title('Performance by Market Regime')\n",
    "                    ax.tick_params(axis='x', rotation=45)\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'No regime data', ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_title('Performance by Market Regime')\n",
    "            else:\n",
    "                ax = axes[1, 0]\n",
    "                ax.text(0.5, 0.5, 'Regime analysis failed', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title('Performance by Market Regime')\n",
    "            \n",
    "            # 4. Prediction stability\n",
    "            if 'prediction_stability' in self.test_results and 'error' not in self.test_results['prediction_stability']:\n",
    "                ax = axes[1, 1]\n",
    "                stab = self.test_results['prediction_stability']\n",
    "                \n",
    "                categories = ['Direction\\nAgreement', 'Prediction\\nCorrelation']\n",
    "                values = [stab.get('mean_direction_agreement', 0.5), stab.get('mean_correlation_between_runs', 0.5)]\n",
    "                \n",
    "                bars = ax.bar(categories, values)\n",
    "                for i, value in enumerate(values):\n",
    "                    color = 'green' if value > 0.8 else 'orange' if value > 0.6 else 'red'\n",
    "                    bars[i].set_color(color)\n",
    "                    ax.text(i, value + 0.02, f'{value:.3f}', ha='center')\n",
    "                \n",
    "                ax.set_ylabel('Score')\n",
    "                ax.set_title('Model Stability Metrics')\n",
    "                ax.set_ylim(0, 1.1)\n",
    "            else:\n",
    "                ax = axes[1, 1]\n",
    "                ax.text(0.5, 0.5, 'Stability analysis failed', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title('Model Stability Metrics')\n",
    "            \n",
    "            # 5. Trading simulation\n",
    "            if 'trading_simulation' in self.test_results and 'error' not in self.test_results['trading_simulation']:\n",
    "                ax = axes[2, 0]\n",
    "                trade = self.test_results['trading_simulation']\n",
    "                \n",
    "                metrics = ['Total Return', 'Win Rate', 'Sharpe Ratio']\n",
    "                values = [trade.get('total_return', 0), trade.get('win_rate', 0.5), trade.get('sharpe_ratio', 0)]\n",
    "                \n",
    "                ax.bar(metrics, values)\n",
    "                ax.set_title('Trading Simulation Results')\n",
    "                ax.set_ylabel('Value')\n",
    "            else:\n",
    "                ax = axes[2, 0]\n",
    "                ax.text(0.5, 0.5, 'Trading simulation failed', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title('Trading Simulation Results')\n",
    "            \n",
    "            # 6. Overall readiness summary\n",
    "            ax = axes[2, 1]\n",
    "            ax.text(0.5, 0.5, 'Overall Readiness Summary\\n(See text report above)', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Trading Readiness Overview')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating plots: {str(e)}\")\n",
    "            print(\"Plots could not be generated, but test results are available in text format above.\")\n",
    "\n",
    "# Example usage:\n",
    "tester = ComprehensiveTradingModelTester(improved_predictor)\n",
    "test_results = tester.run_all_tests(df, save_report=True)\n",
    "tester.plot_test_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc622b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0e95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
